{
  
    
        "post0": {
            "title": "Plotting COVID-19 data",
            "content": "#collapse import numpy as np import librosa import matplotlib.pyplot as plt import pickle from PyEMD import EMD import numpy as np import os import pandas as pd from scipy import signal from sklearn.preprocessing import normalize from scipy.signal import medfilt from scipy.signal import savgol_filter import datetime from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) from mpl_toolkits.axes_grid1 import make_axes_locatable import matplotlib as mpl from datetime import date mpl.rcParams[&#39;figure.dpi&#39;] = 100 . . #collapse path_data = &#39;./my_data/&#39; path_store_figure = &#39;./figures/&#39; # df = pd.read_csv(path_data+&#39;owid-covid-data.csv&#39;) url = &#39;https://covid19.who.int/WHO-COVID-19-global-data.csv&#39; df = pd.read_csv(url) dates = df[&#39;Date_reported&#39;].unique() dates = [datetime.datetime.strptime(ts, &quot;%Y-%m-%d&quot;) for ts in dates] dates.sort() sorteddates = [datetime.datetime.strftime(ts, &quot;%Y-%m-%d&quot;) for ts in dates] data_1 = [] data_2 = [] for i in range(len(dates)): data_1.append(np.sum(df.loc[df[&#39;Date_reported&#39;]==sorteddates[i]][&#39;Cumulative_deaths&#39;].values)) data_2.append(np.sum(df.loc[df[&#39;Date_reported&#39;]==sorteddates[i]][&#39;Cumulative_cases&#39;].values)) countries = df[&#39;Country&#39;].unique() all_country = np.zeros((len(countries),len(sorteddates))) for i in range(len(countries)): temp = df[df[&#39;Country&#39;]==countries[i]] for j in range(len(sorteddates)): temp_1 = temp[temp[&#39;Date_reported&#39;]==sorteddates[j]] if len(temp_1): all_country[i,j] = temp_1[&#39;New_cases&#39;] . . Is there any periodicity over the time? . #collapse S = [] for i in range(all_country.shape[0]): # s = all_country[np.where(countries==&#39;United States of America&#39;)[0][0],:] s = np.nan_to_num(all_country[i,:])+1e-10 s[s&lt;0] = 0 s = s/np.max(np.abs(s)) S.append(s) # take fft nfft = 512 S = np.array(S) S_fft = np.abs(np.fft.rfft(S,nfft,axis=1)) # normalize for i in range(S.shape[0]): S_fft[i,:] = S_fft[i,:]/(np.max(S_fft[i,:])+10e-10) #mean and std S_fft = 20*np.log10(S_fft+10e-20) S_mu = np.mean(S_fft,axis=0) S_std = np.std(S_fft,axis=0)/np.sqrt(all_country.shape[0]) # plot time series fig = plt.subplots(figsize=[6,4]) plt.xkcd() ax = plt.subplot(1,1,1) clr_1 = &#39;tab:blue&#39; ax.plot(np.sum(all_country,axis=0),label=&#39;WORLDWIDE&#39;) ax.plot(all_country[np.where(countries==&#39;United States of America&#39;)[0][0],:],label=&#39;USA&#39;,linewidth=.5) ax.plot(all_country[np.where(countries==&#39;India&#39;)[0][0],:],label=&#39;INDIA&#39;,linewidth=.5) ax.plot(all_country[np.where(countries==&#39;Brazil&#39;)[0][0],:],label=&#39;BRASIL&#39;,linewidth=.5) ax.legend(loc=&#39;upper left&#39;,fontsize=10,frameon=False) ax.grid(axis=&#39;both&#39;, color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=.75,alpha=.1) ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.xaxis.set_ticks_position(&#39;bottom&#39;) ax.set_xlabel(&#39;DAYS SINCE 3rd JAN 2020&#39;) ax.set_ylabel(&#39;DAILY COVID-19 CASES&#39;) # ax.text(.3,-5,&#39;MAG. SPECTRUM OF nDAILY CASES COVID TRACES nAVG. ACROSS 252 COUNTRIES&#39;, # c=clr_1,fontsize=6) fmt = &#39;png&#39; plt.savefig(&#39;/Users/neeks/Desktop/Documents/work/code/python_codes/notebookCodes/coswara/figures/&#39; +&#39;covid_trace_spectrum.&#39;+fmt, dpi=300, format=fmt,transparent=False,bbox_inches=&#39;tight&#39;) plt.show() # plot spectrum fig = plt.subplots(figsize=[6,4]) ax = plt.subplot(1,1,1) fs = 1 faxis = np.arange(0,nfft/2+1)*fs/nfft clr_1 = &#39;tab:blue&#39; ax.plot(faxis,S_mu,c=clr_1,linewidth=2) # ax.plot(faxis,S_mu - S_std,color=&#39;r&#39;,alpha=.3,linewidth=.5) # ax.plot(faxis,S_mu + S_std,color=&#39;r&#39;,alpha=.3,linewidth=.5) ax.vlines(1/7, -35, 0, colors=&#39;gray&#39;, linestyles=&#39;--&#39;,alpha=.3) ax.vlines(2/7, -35, 0, colors=&#39;gray&#39;, linestyles=&#39;--&#39;,alpha=.3) ax.grid(axis=&#39;both&#39;, color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=.75,alpha=.1) ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.xaxis.set_ticks_position(&#39;bottom&#39;) ax.set_xlabel(&#39;FREQUENCY [in 1/DAY]&#39;) ax.set_ylabel(&#39;SPECTRAL POWER [in dB]&#39;) ax.text(.3,-5,&#39;MAG. SPECTRUM OF nDAILY CASES COVID TRACES nAVG. ACROSS 252 COUNTRIES&#39;, c=clr_1,fontsize=6) fmt = &#39;png&#39; plt.savefig(&#39;/Users/neeks/Desktop/Documents/work/code/python_codes/notebookCodes/coswara/figures/&#39; +&#39;covid_trace_time_domain.&#39;+fmt, dpi=300, format=fmt,transparent=False,bbox_inches=&#39;tight&#39;) plt.show() . . Lets&#39; visualize India data . #collapse import json import requests import seaborn as sns from scipy.signal import savgol_filter sns.set(style=&quot;ticks&quot;) mpl.rcParams.update(mpl.rcParamsDefault) # get COVID data from COVID19india.ORG resp = requests.get(&#39;https://api.covid19india.org/data.json&#39;) if resp.status_code != 200: # This means something went wrong. raise ApiError(&#39;GET /tasks/ {}&#39;.format(resp.status_code)) daily_testing = {} daily_testing[&#39;totalsamplestested&#39;] = [] daily_testing[&#39;samplereportedtoday&#39;] = [] daily_testing[&#39;date&#39;] = [] daily_testing[&#39;vaccinated&#39;] = [] for item in resp.json()[&#39;tested&#39;]: if item[&#39;totalsamplestested&#39;]!=&#39;&#39;: daily_testing[&#39;totalsamplestested&#39;].append(int(item[&#39;totalsamplestested&#39;])) else: daily_testing[&#39;totalsamplestested&#39;].append(0) if item[&#39;totalindividualsvaccinated&#39;]!=&#39;&#39;: daily_testing[&#39;vaccinated&#39;].append(int(item[&#39;totalindividualsvaccinated&#39;])) else: daily_testing[&#39;vaccinated&#39;].append(0) # daily_testing[&#39;date&#39;].append(item[&#39;updatetimestamp&#39;]) daily_cases = {} daily_cases[&#39;dailyconfirmed&#39;] = [] daily_cases[&#39;dailydeceased&#39;] = [] daily_cases[&#39;dailyrecovered&#39;] = [] daily_cases[&#39;date&#39;] = [] for item in resp.json()[&#39;cases_time_series&#39;]: daily_cases[&#39;dailyconfirmed&#39;].append(int(item[&#39;dailyconfirmed&#39;])) daily_cases[&#39;dailydeceased&#39;].append(int(item[&#39;dailydeceased&#39;])) daily_cases[&#39;dailyrecovered&#39;].append(int(item[&#39;dailyrecovered&#39;])) daily_cases[&#39;date&#39;].append(item[&#39;date&#39;]) total_cases = {} total_cases[&#39;totalconfirmed&#39;] = [] total_cases[&#39;totaldeceased&#39;] = [] total_cases[&#39;totalrecovered&#39;] = [] total_cases[&#39;date&#39;] = [] for item in resp.json()[&#39;cases_time_series&#39;]: total_cases[&#39;totalconfirmed&#39;].append(int(item[&#39;totalconfirmed&#39;])) total_cases[&#39;totaldeceased&#39;].append(int(item[&#39;totaldeceased&#39;])) total_cases[&#39;totalrecovered&#39;].append(int(item[&#39;totalrecovered&#39;])) total_cases[&#39;date&#39;].append(item[&#39;date&#39;]) . . As occurence-per-second ... . #collapse fig = plt.subplots(figsize=(10,5)) clr_1 = &quot;tab:blue&quot; clr_2 = &quot;tab:orange&quot; clr_3 = &quot;tab:green&quot; ax = plt.subplot(1,1,1) ax.semilogy([24*60*60/i if i&gt;0 else 0 for i in daily_cases[&#39;dailyconfirmed&#39;]],color=clr_1,label=&#39;SOMEONE BECOMES A PATIENT&#39;, basey=2) ax.plot([24*60*60/i if i&gt;0 else 0 for i in daily_cases[&#39;dailyrecovered&#39;]],color=clr_2,label=&#39;SOMEONE RECOVERS&#39;) ax.plot([24*60*60/i if i&gt;0 else 0 for i in daily_cases[&#39;dailydeceased&#39;]],color=clr_3,label=&#39;SOMEONE DIES&#39;) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.tick_params(which=&#39;both&#39;, width=2) ax.grid(axis=&#39;y&#39;, color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=.75,alpha=.1) ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.xaxis.set_ticks_position(&#39;bottom&#39;) ax.spines[&#39;bottom&#39;].set_position((&#39;axes&#39;, -.05)) ax.yaxis.set_ticks_position(&#39;left&#39;) ax.text(300,2**16,&#39;SOMEONE GETS COVID&#39;,fontsize=10,color=clr_1) ax.text(300,2**15,&#39;SOMEONE IS NO MORE&#39;,fontsize=10,color=clr_3) ax.text(300,2**14,&#39;SOMEONE RECOVERS&#39;,fontsize=10,color=clr_2) xticks = [0,55,76,95,109,123,153,184,215,len(total_cases[&#39;totalconfirmed&#39;])-1] cnt = 0 str_lock = [&#39;FIRST CASE&#39;,&#39;LOCKDOWN-1&#39;,&#39;LOCKDOWN-2&#39;,&#39;LOCKDOWN-3&#39;,&#39;LOCKDOWN-4&#39;,&#39;UNLOCK-1&#39;,&#39;UNLOCK-2&#39;,&#39;UNLOCK-3&#39; ,&#39;UNLOCK-4&#39;,&#39; &#39;] xlabels = [] ind_1 = 6 for i in xticks: ax.vlines(i,min(total_cases[&#39;totaldeceased&#39;]),max(total_cases[&#39;totaldeceased&#39;]), linestyles =&quot;--&quot;, colors =&quot;gray&quot;,alpha=0.5,linewidth=.75) ax.vlines(i,min(total_cases[&#39;totaldeceased&#39;]),max(total_cases[&#39;totaldeceased&#39;]), linestyles =&quot;--&quot;, colors =&quot;gray&quot;,alpha=0.5,linewidth=.75) ax.text(x=i-6,y=25000,s=str_lock[cnt],rotation=90,fontsize=7,color=&#39;gray&#39;,alpha=0.6) if i == len(total_cases[&#39;totalconfirmed&#39;]): xlabels.append(total_cases[&#39;date&#39;][-1][:ind_1]) else: xlabels.append(total_cases[&#39;date&#39;][xticks[cnt]][:ind_1]) ax.text(i-6,.3,xlabels[cnt],rotation=90,fontsize=7,color=&#39;gray&#39;,alpha=0.8) cnt = cnt+1 ax.set_ylabel(&#39;EVERY FEW SECONDS IN INDIA&#39;,fontsize=10) ax.set_xlabel(&#39;DAYS SINCE FIRST CASE &#39;,fontsize=10) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax.spines[&#39;bottom&#39;].set_color(&#39;gray&#39;) ax.spines[&quot;left&quot;].set_color(&#39;gray&#39;) ax.tick_params(axis=&#39;x&#39;, colors=&#39;gray&#39;) ax.text(250,.5,&#39;by @neeksww&#39;,fontsize=8,color=&#39;gray&#39;,alpha=0.6) ax.text(250,.25,&#39;data source: https://www.covid19india.org/&#39;,fontsize=8,color=&#39;gray&#39;,alpha=0.6) ax.set_xlim(55,550) fmt = &#39;png&#39; plt.savefig(&#39;/Users/neeks/Desktop/Documents/work/code/python_codes/notebookCodes/coswara/figures/&#39; +&#39;covid_status_time_whole_india.&#39;+fmt, dpi=300, format=fmt,transparent=True,bbox_inches=&#39;tight&#39;) plt.show() . . /Users/neeks/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:58: UserWarning: AutoMinorLocator does not work with logarithmic scale /Users/neeks/miniconda2/envs/py36/lib/python3.6/site-packages/IPython/core/pylabtools.py:128: UserWarning: AutoMinorLocator does not work with logarithmic scale fig.canvas.print_figure(bytes_io, **kw) . As cases ... . #collapse # get vaccination data url = &#39;https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/vaccinations/vaccinations.csv&#39; df = pd.read_csv(url) df = df[df[&#39;location&#39;]==&#39;India&#39;].reset_index(drop=True) vals = df[&#39;total_vaccinations&#39;].values d_0 = date(2020, 1, 30) d_1 = date(int(df[&#39;date&#39;][0].split(&#39;-&#39;)[0]), int(df[&#39;date&#39;][0].split(&#39;-&#39;)[1][1]), int(df[&#39;date&#39;][0].split(&#39;-&#39;)[2])) d_2 = date(int(df[&#39;date&#39;][len(df)-1].split(&#39;-&#39;)[0]), int(df[&#39;date&#39;][len(df)-1].split(&#39;-&#39;)[1][1]), int(df[&#39;date&#39;][len(df)-1].split(&#39;-&#39;)[2])) delta = d_1 - d_0 delta.days daily_cases[&#39;vaccinations&#39;] = [] cnt = 0 for i in range(len(daily_cases[&#39;dailydeceased&#39;])): if i &lt; (d_1-d_0).days: daily_cases[&#39;vaccinations&#39;].append(0) elif i &lt; (d_2-d_0).days: daily_cases[&#39;vaccinations&#39;].append(vals[cnt]) cnt = cnt+1 else: daily_cases[&#39;vaccinations&#39;].append(vals[cnt-1]) daily_cases[&#39;vaccinations&#39;] = np.array([0] + list(np.diff(daily_cases[&#39;vaccinations&#39;]))) daily_cases[&#39;vaccinations&#39;][daily_cases[&#39;vaccinations&#39;]==0] = np.nan #plot everything fig = plt.subplots(figsize=(10,6)) clr_1 = &quot;tab:blue&quot; clr_2 = &quot;tab:orange&quot; clr_3 = &quot;tab:green&quot; clr_4 = &#39;cornflowerblue&#39; LW = 1.0 ax1 = plt.subplot(1,1,1) x_11 = np.arange(0,len(daily_cases[&#39;dailyconfirmed&#39;]),1) y_11 = daily_cases[&#39;dailyconfirmed&#39;] y_12 = daily_cases[&#39;dailyrecovered&#39;] y_max = np.ceil(max(y_11)/1e5)*1e5 ax1.plot(x_11,y_11,color=clr_1,label=&#39;DAILY CASES&#39;, linewidth = LW) ax1.plot(x_11,y_12,color=clr_2,label=&#39;DAILY RECOVERED&#39;, linewidth = LW) ax1.grid(axis=&#39;both&#39;, color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=.75,alpha=.1) ax1.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax1.spines[&#39;bottom&#39;].set_position(&#39;center&#39;) ax1.spines[&quot;left&quot;].set_bounds(0, y_max) ax1.spines[&quot;left&quot;].set_color(&#39;gray&#39;) ax1.spines[&quot;right&quot;].set_bounds(0, 0) ax1.spines[&#39;bottom&#39;].set_color(&#39;gray&#39;) ax1.tick_params(axis=&#39;y&#39;, colors=&#39;gray&#39;) ax1.tick_params(axis=&#39;x&#39;, colors=&#39;gray&#39;) ax1.set_yticks(np.arange(0, y_max,50000)) ax1.set_ylim([-y_max,y_max]) top_pad_1 = 50000 top_pad_2 = 100000 ax1.text(10,y_max-top_pad_1,&#39;INDIA&#39;,fontsize=10,color=&#39;k&#39;) ax1.text(10,y_max-top_pad_2,&#39;DAILY CASES&#39;,fontsize=10,color=clr_1) ax1.text(85,y_max-top_pad_2,&#39;DAILY RECOVERY&#39;,fontsize=10,color=clr_2) ax1.text(180,y_max-top_pad_2,&#39;DAILY DECEASED&#39;,fontsize=10,color=clr_3) ax1.text(275,y_max-top_pad_2,&#39;DAILY VACCINATION&#39;,fontsize=10,color=clr_4) ax1.text(230,-20000,&#39;DAYS&#39;,fontsize=10,color=&#39;k&#39;) ax1.tick_params(axis=&#39;y&#39;, colors=&#39;gray&#39;,direction=&#39;out&#39;) ax2 = ax1.twinx() x_21 = np.arange(0,len(daily_cases[&#39;dailydeceased&#39;]),1) y_21 = daily_cases[&#39;dailydeceased&#39;] y_max = np.ceil(max(y_21)/1e3)*1e3 ax2.plot(x_21,y_21,color=clr_3,label=&#39;DAILY DECEASED&#39;, linewidth = LW) ax2.grid(axis=&#39;both&#39;,color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=.75,alpha=.1) # ax2.xaxis.set_minor_locator(AutoMinorLocator()) # ax2.yaxis.set_minor_locator(AutoMinorLocator()) ax2.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax2.spines[&#39;bottom&#39;].set_position(&#39;center&#39;) ax2.spines[&quot;left&quot;].set_bounds(0, y_max) ax2.spines[&quot;left&quot;].set_bounds(0, 0) ax2.spines[&quot;right&quot;].set_bounds(0, y_max) ax2.spines[&quot;right&quot;].set_position((&#39;axes&#39;,0)) ax2.spines[&#39;bottom&#39;].set_color(&#39;gray&#39;) ax2.spines[&quot;right&quot;].set_color(&#39;gray&#39;) ax2.tick_params(axis=&#39;x&#39;, colors=&#39;gray&#39;) ax2.set_yticks(np.arange(0,y_max,500)[1:]) ax2.set_ylim([-y_max,y_max]) plt.gca().invert_yaxis() ax1.tick_params(axis=&#39;y&#39;, colors=&#39;gray&#39;,direction=&#39;in&#39;, pad=10) ax2.tick_params(axis=&#39;y&#39;, colors=&#39;gray&#39;,direction=&#39;out&#39;, pad=-40) xticks = [0,55,76,95,109,123,153,184,215,len(total_cases[&#39;totalconfirmed&#39;])-1] cnt = 0 str_lock = [&#39;FIRST CASE&#39;,&#39;LOCKDOWN-1&#39;,&#39;LOCKDOWN-2&#39;,&#39;LOCKDOWN-3&#39;,&#39;LOCKDOWN-4&#39;,&#39;UNLOCK-1&#39;,&#39;UNLOCK-2&#39;,&#39;UNLOCK-3&#39; ,&#39;UNLOCK-4&#39;,&#39; &#39;] xlabels = [] ind_1 = 6 for i in xticks: ax1.vlines(i,0,max(daily_cases[&#39;dailyconfirmed&#39;]), linestyles =&quot;--&quot;, colors =&quot;gray&quot;,alpha=0.5,linewidth=.75) ax2.vlines(i,min(total_cases[&#39;totaldeceased&#39;]),max(total_cases[&#39;totaldeceased&#39;]), linestyles =&quot;--&quot;, colors =&quot;gray&quot;,alpha=0.5,linewidth=.75) ax1.text(i-6,250000,str_lock[cnt],rotation=90,fontsize=7,color=&#39;gray&#39;,alpha=0.4) if i == len(total_cases[&#39;totalconfirmed&#39;]): xlabels.append(total_cases[&#39;date&#39;][-1][:ind_1]) else: xlabels.append(total_cases[&#39;date&#39;][xticks[cnt]][:ind_1]) ax1.text(i-6,100000,xlabels[cnt],rotation=90,fontsize=7,color=&#39;gray&#39;,alpha=0.4) cnt = cnt+1 ax1.set_ylabel(&#39;NUMBER OF INDIVIDUALS&#39;,fontsize=10) ax2.text(10,y_max-300,&#39;by @neeksww&#39;,fontsize=8,color=&#39;gray&#39;,alpha=0.6) ax2.text(10,y_max-100,&#39;data source: https://www.covid19india.org/&#39;,fontsize=8,color=&#39;gray&#39;,alpha=0.6) ax3 = ax1.twinx() x_31 = np.arange(0,len(daily_cases[&#39;vaccinations&#39;])) # y_31 = savgol_filter(daily_cases[&#39;vaccinations&#39;],3,1) y_31 = daily_cases[&#39;vaccinations&#39;] y_max = np.ceil(np.max(np.nan_to_num(np.array(y_31)))/1e6)*1e6 LW = 2 ax3.plot(x_31,y_31,color=clr_4,label=&#39;DAILY DECEASED&#39;, alpha=.5, linewidth = LW) ax3.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax3.spines[&#39;bottom&#39;].set_position(&#39;center&#39;) ax3.spines[&quot;left&quot;].set_bounds(0,0) ax3.spines[&quot;right&quot;].set_color(clr_4) ax3.spines[&quot;left&quot;].set_color(&#39;gray&#39;) ax3.spines[&quot;right&quot;].set_bounds(0, y_max) ax3.spines[&#39;bottom&#39;].set_color(&#39;gray&#39;) ax3.tick_params(axis=&#39;y&#39;, colors=clr_4) ax3.set_yticks(np.arange(0, y_max,500000)) ax3.set_ylim([-y_max,y_max]) ax3.set_ylabel(&#39;VACCINATIONS&#39;,fontsize=10,color=clr_4) fmt = &#39;png&#39; plt.savefig(&#39;/Users/neeks/Desktop/Documents/work/code/python_codes/notebookCodes/coswara/figures/&#39; +&#39;covid_status_whole_india.&#39;+fmt, dpi=300, format=fmt,transparent=True,bbox_inches=&#39;tight&#39;) plt.show() . . Is there a pattern on day of week . #collapse dataset = {} dataset[&#39;month&#39;] = [] dataset[&#39;day&#39;] = [] dataset[&#39;year&#39;] = [] dataset[&#39;dailyconfirmed&#39;] = daily_cases[&#39;dailyconfirmed&#39;] dataset[&#39;dailyrecovered&#39;] = daily_cases[&#39;dailyrecovered&#39;] dataset[&#39;dailydeceased&#39;] = daily_cases[&#39;dailydeceased&#39;] # date creation for date in total_cases[&#39;date&#39;]: day = int(date.split(&#39; &#39; )[0]) month = date.split(&#39; &#39;)[1] if month == &#39;January&#39;: month_val = 1 if month == &#39;February&#39;: month_val = 2 if month == &#39;March&#39;: month_val = 3 if month == &#39;April&#39;: month_val = 4 if month == &#39;May&#39;: month_val = 5 if month == &#39;June&#39;: month_val = 6 if month == &#39;July&#39;: month_val = 7 if month == &#39;August&#39;: month_val = 8 if month == &#39;September&#39;: month_val = 9 if month == &#39;October&#39;: month_val = 10 if month == &#39;November&#39;: month_val = 11 if month == &#39;December&#39;: month_val = 12 year_val = 2020 day_val = datetime.datetime(year_val, month_val, day).strftime(&quot;%A&quot;) # push into dict dataset[&#39;day&#39;].append(day_val) dataset[&#39;month&#39;].append(month_val) dataset[&#39;year&#39;].append(year_val) df = pd.DataFrame.from_dict(dataset) label_day = [&#39;Monday&#39;,&#39;Tuesday&#39;,&#39;Wednesday&#39;,&#39;Thursday&#39;,&#39;Friday&#39;,&#39;Saturday&#39;,&#39;Sunday&#39;] fig = plt.subplots(figsize=[9,5]) ax = plt.subplot(1,1,1) day_mu = [] for i in label_day: # if i == &#39;Monday&#39; or i == &#39;Saturday&#39;: temp = df[df[&#39;day&#39;]==i][&#39;dailyconfirmed&#39;].values ax.plot(temp,&#39;-&#39;,label=i[:3],marker=&#39;o&#39;,markeredgecolor=&#39;k&#39;) day_mu.append(np.mean(df[df[&#39;day&#39;]==i][&#39;dailyconfirmed&#39;])) ax.grid(True) ax.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax1.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.1) ax.set_xlabel(&#39;COUNT OF DAY SINCE FIRST COVID-19 CASE&#39;,fontsize=14) ax.set_ylabel(&#39;POSITIVE CASE COUNT&#39;,fontsize=14) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.tick_params(which=&#39;both&#39;, width=2) ax.tick_params(which=&#39;major&#39;, length=7) ax.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax.legend(frameon=False,fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.show() . . Plotting state-wise data . #collapse df = pd.read_csv(&#39;./my_data/covid/tested_numbers_icmr_data.csv&#39;) dates = df[&quot;Tested As Of&quot;].values daily_RTPCR_tets = df[&quot;Daily RTPCR tests&quot;].values temp = df[&quot;Total Samples Tested&quot;].values temp = df[&quot;Total Positive Cases&quot;].values df = pd.read_csv(&#39;./my_data/covid/state_wise_daily.csv&#39;) df = pd.read_csv(&#39;https://api.covid19india.org/csv/latest/state_wise_daily.csv&#39;) # temp_1 = normalize(temp_1,norm=&#39;max&#39;, axis=0) state_labels = [&#39;KA&#39;,&#39;TN&#39;,&#39;MH&#39;,&#39;DL&#39;,&#39;GJ&#39;,&#39;KL&#39;] state_labels = [&#39;MH&#39;,&#39;KA&#39;,&#39;AP&#39;,&#39;UP&#39;,&#39;TN&#39;,&#39;RJ&#39;,&#39;TG&#39;,&#39;AS&#39;,&#39;CH&#39;,&#39;KL&#39;,&#39;DL&#39;,&#39;BR&#39;, &#39;GJ&#39;,&#39;OR&#39;,&#39;WB&#39;,&#39;HR&#39;,&#39;JK&#39;,&#39;HP&#39;,&#39;MP&#39;,&#39;PB&#39;,&#39;NL&#39;,&#39;GA&#39;,&#39;JH&#39;,&#39;AN&#39;, &#39;MN&#39;,&#39;ML&#39;,&#39;SK&#39;,&#39;TR&#39;, ] state_labels_= [&#39;MH&#39;,&#39;KA&#39;,&#39;AP&#39;,&#39;UP&#39;,&#39;TN&#39;,&#39;RJ&#39;,&#39;TS&#39;,&#39;AS&#39;,&#39;CG&#39;,&#39;KL&#39;,&#39;DL&#39;,&#39;BR&#39;, &#39;GJ&#39;,&#39;OD&#39;,&#39;WB&#39;,&#39;HR&#39;,&#39;JK&#39;,&#39;HP&#39;,&#39;MP&#39;,&#39;PB&#39;,&#39;NL&#39;,&#39;GA&#39;,&#39;JH&#39;,&#39;AN&#39;, &#39;MN&#39;,&#39;ML&#39;,&#39;SK&#39;,&#39;TR&#39; ] clr_1 = &#39;tab:blue&#39; clr_2 = &#39;tab:orange&#39; cnt = 0 start_cases = [] start_decea = [] current_cases = [] for i in state_labels: cnt+= 1 start_cases.append(np.where(np.cumsum(df[df[&#39;Status&#39;]==&#39;Confirmed&#39;][i].values)&gt;5)[0][0]) start_decea.append(np.where(np.cumsum(df[df[&#39;Status&#39;]==&#39;Deceased&#39;][i].values)&gt;5)[0][0]) current_cases.append(df[df[&#39;Status&#39;]==&#39;Confirmed&#39;][i].values[-1]) indx = np.argsort(current_cases)[::-1] fig = plt.subplots(figsize=[50,30]) cnt = 0 all_states_cases = [] all_states_decea = [] plt.xkcd() for i in indx: cnt+= 1 ax1 = plt.subplot(4,7,cnt) ax2 = ax1.twinx() temp_1 = medfilt(df[df[&#39;Status&#39;]==&#39;Confirmed&#39;][state_labels[i]].values,3) all_states_cases.append(temp_1/max(temp_1)) # temp_1 = savgol_filter(df[df[&#39;Status&#39;]==&#39;Confirmed&#39;][state_labels[i]].values,window_length=7, polyorder=1, # deriv=0, delta=1.0, axis=0, mode=&#39;nearest&#39;) temp_2 = medfilt(df[df[&#39;Status&#39;]==&#39;Deceased&#39;][state_labels[i]].values,3) all_states_decea.append(temp_2/(max(temp_2))) # temp_2 = savgol_filter(df[df[&#39;Status&#39;]==&#39;Deceased&#39;][state_labels[i]].values,window_length=7, polyorder=1, # deriv=0, delta=1.0, axis=0, mode=&#39;nearest&#39;) ax1.plot(np.arange(0,len(temp_1),1),temp_1,color=clr_1, linewidth=3) ax1.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax1.spines[&#39;left&#39;].set_color(&#39;none&#39;) ax1.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax1.spines[&#39;bottom&#39;].set_color(&#39;gray&#39;) ax1.tick_params(axis=&#39;y&#39;, colors=clr_1) ax2.tick_params(axis=&#39;x&#39;, colors=&#39;gray&#39;) ax1.text(0,max(temp_1)-0.0*max(temp_1),&#39;Indian State: &#39;+state_labels_[i],fontsize=12,color=&#39;tab:red&#39;,alpha=0.9, fontweight=&#39;bold&#39;) # , bbox=dict(facecolor=&#39;gray&#39;, alpha=0.2)) ax1.text(0,max(temp_1)-0.1*max(temp_1),&#39;daily cases&#39;,fontsize=14,color=clr_1) ax1.text(0,max(temp_1)-0.15*max(temp_1),&#39;daily deceased&#39;,fontsize=14,color=clr_2) ax1.text(0,max(temp_1)-0.3*max(temp_1),&#39;For days since: n&#39;+ df[&#39;Date&#39;][0].split(&#39;-20&#39;)[0] +&#39; to &#39;+ df[&#39;Date&#39;][len(df)-1].split(&#39;-20&#39;)[0],fontsize=14,color=&#39;black&#39;,alpha=0.75) ax1.text(0,max(temp_1)-0.5*max(temp_1),&#39;With three sample n median filtered&#39;,fontsize=10,color=&#39;gray&#39;, rotation=0,alpha=0.6) ax1.text(0,max(temp_1)-0.9*max(temp_1),&#39;data source: n https://www.covid19india.org/&#39;,fontsize=10,color=&#39;gray&#39;, rotation=0,alpha=0.4) if cnt == 4: ax1.text(len(temp_1)-10,0,&#39;@neeksww&#39;,fontsize=14,color=&#39;gray&#39;, rotation=90,alpha=0.3) ax1.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.1) ax2.plot(np.arange(0,len(temp_2),1),temp_2,color=clr_2, linewidth=3) ax2.tick_params(axis=&#39;x&#39;, labelcolor=clr_2) ax2.spines[&#39;top&#39;].set_color(&#39;none&#39;) ax2.spines[&#39;right&#39;].set_color(&#39;none&#39;) ax2.spines[&#39;left&#39;].set_color(&#39;none&#39;) ax2.spines[&#39;bottom&#39;].set_color(&#39;gray&#39;) ax2.tick_params(axis=&#39;y&#39;, colors=clr_2) # clr_face = (79/256,178/256,151/256) # ax1.set_facecolor(clr_face) # ax2.set_facecolor(clr_face) fmt = &#39;png&#39; plt.savefig(&#39;/Users/neeks/Desktop/Documents/work/code/python_codes/notebookCodes/coswara/figures/&#39; +&#39;covid_status_india.&#39;+fmt, dpi=300, format=fmt,transparent=False,bbox_inches=&#39;tight&#39;) plt.show() . . #collapse if 0: len(all_states_cases[0]) x = signal.correlate(all_states_cases[0],all_states_cases[4]) print(396-np.argmax(x)) print(x.shape) plt.plot(x.T) plt.show() fig, ax = plt.subplots(figsize=[12,6]) for i in range(len(all_states_cases)): plt.plot(all_states_cases[i]) plt.show() fig, ax = plt.subplots(figsize=[12,6]) im = ax.imshow(all_states_cases,origin=&#39;lower&#39;,aspect=&#39;auto&#39;,extent = [0,temp_1.shape[0], 0, len(all_states_cases)], cmap=&#39;Blues&#39;,vmin = 0, vmax =1) divider = make_axes_locatable(ax) colorbar_ax = fig.add_axes([.95, 0.1, 0.015, 0.5]) fig.colorbar(im, cax=colorbar_ax) plt.show() fig, ax = plt.subplots(figsize=[12,6]) im = ax.imshow(all_states_decea,origin=&#39;lower&#39;,aspect=&#39;auto&#39;,extent = [0,temp_1.shape[0], 0, len(all_states_cases)], cmap=&#39;Blues&#39;,vmin = 0, vmax =1) divider = make_axes_locatable(ax) colorbar_ax = fig.add_axes([.95, 0.1, 0.015, 0.5]) fig.colorbar(im, cax=colorbar_ax) plt.show() . . Thats&#39; it for now! .",
            "url": "https://neerajww.github.io/myblog/2021/06/24/plot_covid19.html",
            "relUrl": "/2021/06/24/plot_covid19.html",
            "date": " â€¢ Jun 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Faculty base in few Indian Institutes",
            "content": "About . Ocassionally, I get the urge to know more about the Indian Institutes of National Importance. This time, the urge was fuelled on seeing the NIRF 2021 datasheet prepared and hosted by some of the institutes on their website. Starring at the datasheets, I thought its better to visualize it. Below is the result. You may find it particularly useful for . understanding the institutes on dimensions shown in the image below | evaluating the institutes if you are a faculty aspirant | cheering up and/or bringing a change | . . Note that this presentation has my own bias of visualizing only along certain dimensions. I encourage you to go deeper if you are interested on something I dont discuss below. I will share the datasheet with you. . . Step 1: I collected the NIRF 2021 data hosted by few institutes of our interest. Note that this is 2021 data, and as I understand, this has been submitted to the Ministry of Education and hence, we can consider them authentic. The NIRF 2021 rankings are yet not released. . Thanks to the meticulous data preparation by few insitutes, I could easily download this data from the institute websites. We will focus on 13 institutes. These are: . Indian Institute of Science (iisc) | IIT Bombay (iit_bom), IIT Kharagpur (iit_kgp), IIT Madras (iit_mad), IIT Kanpur (iit_kan), IIT Delhi (iit_delhi), IIT Varansi (iit_bhu), IIT Roorkee (iit_roor) | IIT Jodhpur (iit_jodhpur), IIT Indore (iit_ind), IIT Bhubaneswar (iit_bbs), IIT Mandi (iit_mandi), IIT Hyderabad (iit_hyd) | . Below you can see these institute locations overlaid on a map. The idea of IIT was conceived around 1947 to enable India&#39;s technology focussed societal progress after the second world war. It was decided to create one of these in north, south, east and west of India. However, now we have them spread across many more places, with most of the states having one IIT (not all are shown in the map below). This is indeed very good as it helps to cater to the large population of India. . #collapse import PyPDF2 import glob import numpy as np import pandas as pd from geopy.geocoders import Nominatim import matplotlib.pyplot as plt plt.rcParams.update({&#39;font.size&#39;: 12}) from mpl_toolkits.basemap import Basemap import seaborn as sns sns.set() # Use seaborn&#39;s default style to make attractive graphs sns.set_style(&quot;white&quot;) sns.set_style(&quot;ticks&quot;) institutes = [&#39;iisc&#39;,&#39;iit_kgp&#39;,&#39;iit_bhu&#39;,&#39;iit_delhi&#39;,&#39;iit_mad&#39;,&#39;iit_bom&#39;,&#39;iit_kan&#39;,&#39;iit_roor&#39;, &#39;iit_ind&#39;,&#39;iit_hyd&#39;,&#39;iit_mandi&#39;,&#39;iit_jodh&#39;,&#39;iit_bbs&#39;] # make map geolocator = Nominatim(user_agent=&quot;geoapiExercises&quot;) def geolocate(country): try: # Geolocate the center of the country loc = geolocator.geocode(country) # And return latitude and longitude return (loc.latitude, loc.longitude) except: # Return missing value return np.nan city_names = [&#39;bangalore&#39;, &#39;kharagpur&#39;,&#39;varanasi&#39;,&#39;delhi&#39;,&#39;chennai&#39;,&#39;mumbai&#39;,&#39;kanpur&#39;,&#39;roorkee&#39;, &#39;indore&#39;,&#39;hyderabad&#39;,&#39;mandi&#39;,&#39;jodhpur&#39;,&#39;bhubaneswar&#39;] df = {} df[&#39;city_name&#39;] = city_names df = pd.DataFrame.from_dict(df) df[&#39;lat&#39;] = 0 df[&#39;long&#39;] = 0 for i in range(len(df)): temp = geolocate(df[&#39;city_name&#39;][i]) df.loc[i,&#39;lat&#39;] = temp[0] df.loc[i,&#39;long&#39;] = temp[1] # Make the india map and overlay locations fig = plt.subplots(figsize=[12,12]) # m = Basemap(llcrnrlon=-180, llcrnrlat=-65, urcrnrlon=180, urcrnrlat=80) m = Basemap(llcrnrlon=60, llcrnrlat=5, urcrnrlon=100, urcrnrlat=37) m.drawmapboundary(fill_color=&#39;#FFF&#39;, linewidth=0) m.fillcontinents(color=&#39;grey&#39;, alpha=0.25) m.drawcoastlines(linewidth=0.1, color=&quot;white&quot;) # prepare a color for each point depending on the continent. df[&#39;labels_enc&#39;] = pd.factorize(df[&#39;city_name&#39;])[0] # Add a point per position m.scatter(x=df[&#39;long&#39;], y=df[&#39;lat&#39;], s=50,alpha=1, c=df[&#39;labels_enc&#39;]/100, cmap=&quot;Set1&quot;) for i in range(len(df)): x = df.loc[i,&#39;long&#39;]+0.5 y = df.loc[i,&#39;lat&#39;] plt.text(x, y, institutes[i]) plt.show() . . Step 2: I did little bit of coding (in python) exercise to extract data from the PDF documents collected in Step 1. This data extracttion step is critical as it helps to load the data into a plotting tool! . Step 3: Rest is just plotting (again, using python). . Note: I will be analyzing data of faculty at three levels, namely, Assistant, Associate, and Full Professor. Also, I have skipped the Institute Director (1 nos.) from the analysis. . So, what do we see! . . #collapse # path to PDFs path_nirf = &#39;/Users/neeks/Desktop/Documents/work/code/python_codes/notebookCodes/institutes/data/nirf_pdfs/&#39; # some function defs def get_curated_rows(pdfReader): # get faculty listing start page for i in range(pdfReader.numPages): pageObj = pdfReader.getPage(i) lines = pageObj.extractText() if &#39;Faculty Details&#39; in lines: page_indx = i break all_rows = [] for i in range(page_indx,pdfReader.numPages): pageObj = pdfReader.getPage(i) if i == page_indx: lines = pageObj.extractText().split(&#39;Association type&#39;)[1] else: lines = pageObj.extractText() line_breaks = [&#39;Regular&#39;, &#39;Visiting&#39;, &#39;Other&#39;] temp = lines for text in line_breaks: temp = temp.replace(text,&#39;;&#39;) all_rows.extend(temp.split(&#39;;&#39;)) # remove contractual rows curated_rows = [] for text in all_rows: if len(text) == 0: continue elif &#39;Adhoc&#39; in text: temp = text.split(&#39;Adhoc /Contractual&#39;) for j in range(len(temp)): temp_1 = temp[j] if &#39;Professor&#39; in temp_1: curated_rows.append(temp_1) else: if &#39;Professor&#39; in text: curated_rows.append(text) return curated_rows def get_parsed_dict(curated_rows, code): # make dictionary keys = [&#39;name&#39;,&#39;age&#39;,&#39;designation&#39;,&#39;gender&#39;,&#39;degree&#39;,&#39;months&#39;,&#39;joining_day&#39;, &#39;joining_month&#39;,&#39;joining_year&#39;,&#39;left&#39;,&#39;institute&#39;] data_dict = {} for key in keys: data_dict[key] = [] for text in curated_rows: # remove serial number for i in range(len(text)): if (i == 0) &amp; (text[i]==&#39; &#39;): continue if not(text[i].isdigit()): break text = text[i:] # search name for i in range(len(text)): if text[i].isdigit(): break data_dict[&#39;name&#39;].append(text[:i]) text = text[i:] # search age for i in range(len(text)): if not(text[i].isdigit()): break data_dict[&#39;age&#39;].append(int(text[:i])) text = text[i:] # search designation text = text.split(&#39;Professor&#39;) if len(text[0]) == 0: data_dict[&#39;designation&#39;].append(&#39;Professor&#39;) else: data_dict[&#39;designation&#39;].append(text[0]) text = text[1] # search for gender, degree if &#39;Ph.D&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;Ph.D&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;Ph.D&#39;) text = text.split(&#39;Ph.D&#39;)[1] elif &#39;M.Tech&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;M.Tech&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;M.Tech&#39;) text = text.split(&#39;M.Tech&#39;)[1] elif &#39;B.Tech&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;B.Tech&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;B.Tech&#39;) text = text.split(&#39;B.Tech&#39;)[1] elif &#39;Master of Design&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;Master of Design&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;M.Des&#39;) text = text.split(&#39;Master of Design&#39;)[1] elif &#39;PGID&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;PGID&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;PGID&#39;) text = text.split(&#39;PGID&#39;)[1] elif &#39;PGDBM&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;PGDBM&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;PGDBM&#39;) text = text.split(&#39;PGDBM&#39;)[1] elif &#39;M.Sc.(Engg)&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;M.Sc.(Engg)&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;M.Sc.(Engg)&#39;) text = text.split(&#39;M.Sc.(Engg)&#39;)[1] elif &#39;M.Sc.&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;M.Sc.&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;M.Sc.&#39;) text = text.split(&#39;M.Sc.&#39;)[1] elif &#39;MFA(Fine Arts)&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;MFA(Fine Arts)&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;MFA(Fine Arts)&#39;) text = text.split(&#39;MFA(Fine Arts)&#39;)[1] elif &#39;P.G.Diploma&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;P.G.Diploma&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;P.G.Diploma&#39;) text = text.split(&#39;P.G.Diploma&#39;)[1] elif &#39;M. Phil&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;M. Phil&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;M. Phil&#39;) text = text.split(&#39;M. Phil&#39;)[1] elif &#39;M.Arch.&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;M.Arch.&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;M.Arch.&#39;) text = text.split(&#39;M.Arch.&#39;)[1] elif &#39;MBA&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;MBA&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;MBA&#39;) text = text.split(&#39;MBA&#39;)[1] elif &#39;M.A&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;M.A&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;M.A&#39;) text = text.split(&#39;M.A&#39;)[1] elif &#39;M.S&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;M.S&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;M.S&#39;) text = text.split(&#39;M.S&#39;)[1] elif &#39;M.E.&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;M.E.&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;M.E.&#39;) text = text.split(&#39;M.E.&#39;)[1] elif &#39;B.E&#39; in text: data_dict[&#39;gender&#39;].append(text.split(&#39;B.E&#39;)[0]) data_dict[&#39;degree&#39;].append(&#39;B.E&#39;) text = text.split(&#39;B.E&#39;)[1] # search months for i in range(len(text)): if (i == 0) &amp; (text[i]==&#39; &#39;): continue if not(text[i].isdigit()): break data_dict[&#39;months&#39;].append(int(text[:i])) text = text[i:] # joining date for i in range(len(text)): if text[i].isdigit(): break text = text[i:] data_dict[&#39;joining_day&#39;].append(int(text.split(&#39;-&#39;)[0])) data_dict[&#39;joining_month&#39;].append(int(text.split(&#39;-&#39;)[1])) data_dict[&#39;joining_year&#39;].append(int(text.split(&#39;-&#39;)[2][:4])) # left or continues text = text.split(&#39;-&#39;)[2][4:] if len(text)&gt;0: if text[0].isdigit(): data_dict[&#39;left&#39;].append(1) else: data_dict[&#39;left&#39;].append(0) # institute code data_dict[&#39;institute&#39;].append(code) return data_dict def get_admin_info(pdfReader, code): admin_data = {} keys = [&#39;PhDs_full&#39;,&#39;PhDs_part&#39;, &#39;staff_salaries&#39;, &#39;institute&#39;] for key in keys: admin_data[key] = [] # search for PhD details page for i in range(pdfReader.numPages): pageObj = pdfReader.getPage(i) lines = pageObj.extractText() if &#39;Ph.D Student Details&#39; in lines: page_indx = i break pageObj = pdfReader.getPage(i) lines = pageObj.extractText() text = lines.split(&quot;Total StudentsFull Time&quot;)[1] # get PhD Full time count for i in range(len(text)): if (i == 0) &amp; (text[i]==&#39; &#39;): continue if not(text[i].isdigit()): break admin_data[&#39;PhDs_full&#39;].append(int(text[:i])) text = text[i:].split(&#39;Part Time&#39;)[1] # get PhD Part time count for i in range(len(text)): if (i == 0) &amp; (text[i]==&#39; &#39;): continue if not(text[i].isdigit()): break admin_data[&#39;PhDs_part&#39;].append(int(text[:i])) # search for salaries page for i in range(pdfReader.numPages): pageObj = pdfReader.getPage(i) lines = pageObj.extractText() if &#39;Salaries (Teaching and Non Teaching staff)&#39; in lines: page_indx = i break pageObj = pdfReader.getPage(i) lines = pageObj.extractText() text = lines.split(&quot;Salaries (Teaching and Non Teaching staff)&quot;)[1] # get PhD Full time count for i in range(len(text)): if (i == 0) &amp; (text[i]==&#39; &#39;): continue if not(text[i].isdigit()): break admin_data[&#39;staff_salaries&#39;].append(int(text[:i])) admin_data[&#39;institute&#39;].append(code) return admin_data def remove_items(test_list, item): # using list comprehension to perform the task res = [i for i in test_list if i != item] return res def name_filter(name): temp = name.split() if len(temp[0])&lt;3: temp_1 = temp[1] else: if (temp[0] == &#39;SMT&#39;) | (temp[0] == &#39;Prof&#39;): temp_1 = temp[1] else: temp_1 = temp[0] return temp_1 # main code starts here institutes = [&#39;iisc&#39;,&#39;iit_kgp&#39;,&#39;iit_bhu&#39;,&#39;iit_delhi&#39;,&#39;iit_mad&#39;,&#39;iit_bom&#39;,&#39;iit_kan&#39;,&#39;iit_roor&#39;, &#39;iit_ind&#39;,&#39;iit_hyd&#39;,&#39;iit_mandi&#39;,&#39;iit_jodh&#39;,&#39;iit_bbs&#39;] cnt = 0 for insitute in institutes: file_name = glob.glob(path_nirf+insitute+&#39;*&#39;)[0] pdfFileObj = open(file_name, &#39;rb&#39;) pdfReader = PyPDF2.PdfFileReader(pdfFileObj) # get admmin df data = get_admin_info(pdfReader, insitute) df_admin_temp = pd.DataFrame.from_dict(data) if cnt == 0: df_admin = df_admin_temp.copy() else: df_admin = pd.concat([df_admin, df_admin_temp]) # get faculty df rows = get_curated_rows(pdfReader) data = get_parsed_dict(rows, insitute) df_fac_temp = pd.DataFrame.from_dict(data) if cnt == 0: df_fac = df_fac_temp.copy() else: df_fac = pd.concat([df_fac, df_fac_temp]) cnt = cnt+1 . . How is the PhD Scholars count ... . #collapse # plot PhD count data = {} keys_1 = [&#39;PhDs_full&#39;,&#39;PhDs_part&#39;] with plt.xkcd(): fig = plt.subplots(figsize=[16,8]) ax = plt.subplot(1,1,1) FS = 14 clr = [&#39;cornflowerblue&#39;,&#39;lightcoral&#39;] for i in range(len(keys_1)): if i == 0: ax.bar(np.arange(len(institutes))+0.1*i,df_admin[&#39;PhDs_full&#39;],width=0.3,color=clr[i], label=keys_1[i]+str(&#39;-time&#39;)) else: ax.bar(np.arange(len(institutes))+0.1*i,df_admin[&#39;PhDs_part&#39;],width=0.3,color=clr[i], label=keys_1[i]+str(&#39;-time&#39;)) plt.xticks(np.arange(0,len(institutes))+.1,institutes,rotation=0, fontsize=FS) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.legend(frameon=False,fontsize=FS) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.set_ylabel(&#39;SCHOLAR COUNT n (as of 2020-21)&#39;, fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # fmt = &#39;.pdf&#39; # if fig_save: # ax.figure.savefig(path_store_figure+&quot;dicova_track_2_dur&quot;+fmt, bbox_inches=&#39;tight&#39;) plt.show() . . I used to think that IITs cater mostly to undergraduate education. After seeing the plot, I was wrong. Some of the IITs have close to (or more than) 2500 PhD Scholars (in 2020). This is huge, and even more than that of IISc, a leading research institute of India. With a mix of good undergraduate and PhD Scholar population in these IITs, they are in a unique position to offer a dynamic and happening environment for research and education. | IIT-BHU is falling behind in PhD Scholars count. I don&#39;t know why. The new IITs (ii_hyd, iit_ind, iit_jodh, iit_bbs, iit_mandi) were established in 2008-09, and exhibit a good count of PhD Scholars. Hopefully, this will increase. iit_hyd is scaling up really well | Part-time PhD Scholars are mainly in institutes located in metros (except, iit_roor). Is it because of easy accessibility? | . . How is the Faculty count... . #collapse # plot all salary exoense data = {} keys_1 = [&#39;staff_salaries&#39;] val_fac_count = [] for institute in institutes: val_fac_count.append(len(df_fac[(df_fac[&#39;institute&#39;]==institute) &amp; (df_fac[&#39;left&#39;]==0)])) with plt.xkcd(): fig = plt.subplots(figsize=[16,7]) ax = plt.subplot(1,1,1) FS = 14 clr = [&#39;cornflowerblue&#39;] ax.bar(np.arange(len(institutes))+0.25,val_fac_count,width=0.3,color=clr[0]) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) ax.set_ylabel(&#39;FACULTY COUNT (Assis.+Assoc.+Full Prof.) n (as of Dec. 2020)&#39;) plt.xticks(np.arange(0,len(institutes))+.1,institutes,rotation=0, fontsize=FS) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&quot;right&quot;].set_visible(False) # fmt = &#39;.pdf&#39; # if fig_save: # ax.figure.savefig(path_store_figure+&quot;dicova_track_2_dur&quot;+fmt, bbox_inches=&#39;tight&#39;) plt.show() . . The well established institutes have more than 400 faculty (just to remind, I am counting Asst.+Assoc.+Prof. only). . Not all old IITs have same count of faculty. The iit_kgp and iit_bom shoot close to 700! Why is iit_kan lagging in this? | Although IISc is lower on undergraduate student count (not shown here), the faculty strength is (relatively) quite good, at close to 450. | Again, amongst the new IITs, iit_hyd is nicely standing tall. | . . Lets look at facult count and salary expense ... jointly . I collected the data on salary expense (teaching and non-teaching). I don&#39;t have information on which all staff members this includes. I still tried plotting them alongside the faculty count. As expected, there is a positive correlation. . #collapse # plot all salary exoense data = {} keys_1 = [&#39;staff_salaries&#39;] val_fac_count = [] for institute in institutes: val_fac_count.append(len(df_fac[(df_fac[&#39;institute&#39;]==institute)&amp; (df_fac[&#39;left&#39;]==0)])) with plt.xkcd(): fig = plt.subplots(figsize=[10,10]) ax = plt.subplot(1,1,1) FS = 14 clr = [&#39;cornflowerblue&#39;,&#39;lightcoral&#39;] for i in range(len(institutes)): ax.scatter(df_admin[&#39;staff_salaries&#39;].values[i]/1e7,val_fac_count[i],s=100,alpha=.75) plt.text(df_admin[&#39;staff_salaries&#39;].values[i]/1e7+np.random.randint(-5,5),val_fac_count[i]+np.random.randint(0,10), institutes[i],fontsize=FS-2, rotation=np.random.randint(-90,90)) ax.plot([0,700],[0,700],&#39;--&#39;,c=&#39;black&#39;,alpha=0.5) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.set_xlabel(&#39;SALARY EXPENSE (teaching and non-teaching) [in Cr. INR]&#39;) ax.set_ylabel(&#39;FACULTY COUNT (Assis.+Assoc.+Full Prof.)&#39;) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) ax.set_xlim([0,750]) ax.set_ylim([0,750]) # fmt = &#39;.pdf&#39; # if fig_save: # ax.figure.savefig(path_store_figure+&quot;dicova_track_2_dur&quot;+fmt, bbox_inches=&#39;tight&#39;) plt.show() . . Is iit_kgp being underpaid? | or, is it that iit_mad and iisc have a huge staff count outside the faculty pool of Asst.+ Assoc.+Full Prof. | . . Lets look at age distribution of Faculty ... . I now shift gears, and plot the age distribution in the faculty pool. . #collapse # exclude faculty who have left df = df_fac[df_fac[&#39;left&#39;]==0].copy() # plot age (without gender) data = {} keys = [&#39;age&#39;,&#39;gender&#39;,&#39;designation&#39;] for key in keys: data[key] = [] for institute in institutes: for key in keys: data[key].append(df[(df[&#39;institute&#39;]==institute)][key].values) with plt.xkcd(): fig = plt.subplots(figsize=[16,7]) ax = plt.subplot(1,1,1) FS = 14 sns.boxplot(data = data[&#39;age&#39;], whis = np.inf,width = 0.2,color=(.3,.8,1),palette=&quot;muted&quot;) sns.swarmplot(data = data[&#39;age&#39;], color=&#39;gray&#39;,alpha=0.4,palette=&quot;muted&quot;) # sns.violinplot(data = data[&#39;age&#39;], palette=&quot;muted&quot;) plt.xticks(np.arange(0,len(institutes)),institutes,rotation=0, fontsize=FS-2) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.set_ylabel(&#39;AGE [in yrs]&#39;, fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # fmt = &#39;.pdf&#39; # if fig_save: # ax.figure.savefig(path_store_figure+&quot;dicova_track_2_dur&quot;+fmt, bbox_inches=&#39;tight&#39;) plt.show() . . Some observations: . Good spread, from 30s to 65s. In case you are not aware, the retirement age is 65. | As expected, new IITs have a relatively younger faculty population. | Why do iit_jodh and iit_bbs have outliers going beyond 65? We will come to this later. | . How about age of just Assistant Professors ... youngistaan! . #collapse # plot age (without gender) data = {} keys = [&#39;age&#39;,&#39;gender&#39;,&#39;designation&#39;] for key in keys: data[key] = [] for institute in institutes: for key in keys: data[key].append(df[(df[&#39;institute&#39;]==institute)&amp;(df[&#39;designation&#39;]==&#39;Assistant&#39;)][key].values) with plt.xkcd(): fig = plt.subplots(figsize=[16,7]) ax = plt.subplot(1,1,1) FS = 14 sns.boxplot(data = data[&#39;age&#39;], whis = np.inf,width = 0.2,color=(.3,.8,1),palette=&quot;muted&quot;) sns.swarmplot(data = data[&#39;age&#39;], color=&#39;gray&#39;,alpha=0.4,palette=&quot;muted&quot;) # sns.violinplot(data = data[&#39;age&#39;], palette=&quot;muted&quot;) plt.xticks(np.arange(0,len(institutes)),institutes,rotation=0, fontsize=FS-2) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.set_ylabel(&#39;AGE [in yrs]&#39;, fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # fmt = &#39;.pdf&#39; # if fig_save: # ax.figure.savefig(path_store_figure+&quot;dicova_track_2_dur&quot;+fmt, bbox_inches=&#39;tight&#39;) plt.show() . . age of Associate Professors ... . #collapse # plot age (without gender) data = {} keys = [&#39;age&#39;,&#39;gender&#39;,&#39;designation&#39;] for key in keys: data[key] = [] for institute in institutes: for key in keys: data[key].append(df[(df[&#39;institute&#39;]==institute)&amp;(df[&#39;designation&#39;]==&#39;Associate&#39;)][key].values) with plt.xkcd(): fig = plt.subplots(figsize=[16,7]) ax = plt.subplot(1,1,1) FS = 14 sns.boxplot(data = data[&#39;age&#39;], whis = np.inf,width = 0.2,color=(.3,.8,1),palette=&quot;muted&quot;) sns.swarmplot(data = data[&#39;age&#39;], color=&#39;gray&#39;,alpha=0.4,palette=&quot;muted&quot;) # sns.violinplot(data = data[&#39;age&#39;], palette=&quot;muted&quot;) plt.xticks(np.arange(0,len(institutes)),institutes,rotation=0, fontsize=FS-2) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.set_ylabel(&#39;AGE [in yrs]&#39;, fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # fmt = &#39;.pdf&#39; # if fig_save: # ax.figure.savefig(path_store_figure+&quot;dicova_track_2_dur&quot;+fmt, bbox_inches=&#39;tight&#39;) plt.show() . . and, age of (Legendary) Professors ... . #collapse # plot age (without gender) data = {} keys = [&#39;age&#39;,&#39;gender&#39;,&#39;designation&#39;] for key in keys: data[key] = [] for institute in institutes: for key in keys: data[key].append(df[(df[&#39;institute&#39;]==institute)&amp;(df[&#39;designation&#39;]==&#39;Professor&#39;)][key].values) with plt.xkcd(): fig = plt.subplots(figsize=[16,7]) ax = plt.subplot(1,1,1) FS = 14 sns.boxplot(data = data[&#39;age&#39;], whis = np.inf,width = 0.2,color=(.3,.8,1),palette=&quot;muted&quot;) sns.swarmplot(data = data[&#39;age&#39;], color=&#39;gray&#39;,alpha=0.4,palette=&quot;muted&quot;) # sns.violinplot(data = data[&#39;age&#39;], palette=&quot;muted&quot;) plt.xticks(np.arange(0,len(institutes)),institutes,rotation=0, fontsize=FS-2) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.set_ylabel(&#39;AGE [in yrs]&#39;, fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # fmt = &#39;.pdf&#39; # if fig_save: # ax.figure.savefig(path_store_figure+&quot;dicova_track_2_dur&quot;+fmt, bbox_inches=&#39;tight&#39;) plt.show() . . Some observations: . iit_mandi is yet to feature a Full Professor. I have excluded the Dean here. The quality of education and research the institute has attained even without this is impressive! | iit_jodh and iit_bbs has relatively more senior Full Professors. Some go beyond 65. In the datasheet these are categorised as Visiting. | iit_ind and iit_hyd feature a group of young Full Professors. | . . How is Faculty count across 3 designations ... . In Indian Academia there are three designations for a faculty position, namely, Assistant, Associate, and Full Professor. This is similar to US Academia. Lets&#39; see how is the count across these three designations. Some observations. . #collapse # plot deisgnation (with gender) data = {} keys_1 = [&#39;Male&#39;,&#39;Female&#39;] keys_2 = [&#39;Assistant&#39;,&#39;Associate&#39;,&#39;Professor&#39;] for key_1 in keys_1: data[key_1] = {} for key_2 in keys_2: data[key_1][key_2] = [] for institute in institutes: for key_1 in keys_1: for key_2 in keys_2: data[key_1][key_2].append(len(df[(df[&#39;institute&#39;]==institute) &amp; (df[&#39;gender&#39;]==key_1) &amp; (df[&#39;designation&#39;]==key_2)])) with plt.xkcd(): fig = plt.subplots(figsize=[16,7]) ax = plt.subplot(1,1,1) FS = 14 clr = [&#39;cornflowerblue&#39;,&#39;tab:red&#39;,&#39;tab:cyan&#39;] for i in range(len(keys_1)): for j in range(len(keys_2)): if i == 0: ax.bar(np.arange(len(institutes))+0.1*i+0.2*j,data[keys_1[i]][keys_2[j]],width=0.1,color=clr[j], label=keys_2[j]+str(&#39; (M)&#39;)) else: ax.bar(np.arange(len(institutes))+0.1*i+0.2*j,data[keys_1[i]][keys_2[j]],width=0.1,color=clr[j], hatch=&#39;//&#39;,label=keys_2[j]+str(&#39; (F)&#39;)) plt.xticks(np.arange(0,len(institutes))+.25,institutes,rotation=0, fontsize=FS) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.legend(frameon=False,fontsize=FS) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.set_ylabel(&#39;COUNT&#39;, fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # fmt = &#39;.pdf&#39; # if fig_save: # ax.figure.savefig(path_store_figure+&quot;dicova_track_2_dur&quot;+fmt, bbox_inches=&#39;tight&#39;) plt.show() . . For a well established institute, (Asst. nos.) &lt; (Assoc. nos.) &lt; (Full Prof. nos) will likely indicate the institute is recruiting, promoting, and retiring in a timely manner. Example, iit_mad! | Every institute may have their own strategy for recruitment and promotion. Some institute recruit in burst or promote based on lots of checks and bounds. These things might impact the unequal count in faculty across designations. | New IITs, as these are being established, will feature a higher Asst. nos. But wait, why are iit_bbs and iit_jodh Assoc. nos count relatively low? We will see that for iit_jodh atleast, the reason is the increased recent intake. | . . Look at the gender ratio ... . You would have noticed the striking difference between male and female faculty count. We can see the same story in the gender ratio. . #collapse # plot deisgnation (with gender) data = {} keys_1 = [&#39;Male&#39;,&#39;Female&#39;] for key_1 in keys_1: data[key_1] = [] for institute in institutes: for key_1 in keys_1: data[key_1].append(len(df[(df[&#39;institute&#39;]==institute) &amp; (df[&#39;gender&#39;]==key_1)])) with plt.xkcd(): fig = plt.subplots(figsize=[16,7]) ax = plt.subplot(1,1,1) FS = 14 clr = [&#39;cornflowerblue&#39;,&#39;tab:red&#39;,&#39;tab:cyan&#39;] ax.bar(np.arange(len(institutes)),np.array(data[keys_1[0]])/np.array(data[keys_1[1]]),width=0.4,color=clr[0], hatch=&#39;//&#39;+str(&#39; (F)&#39;)) plt.xticks(np.arange(0,len(institutes)),institutes,rotation=0, fontsize=FS) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.set_ylabel(&#39;MALE-to-FEMALE FACULTY RATIO&#39;, fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # fmt = &#39;.pdf&#39; # if fig_save: # ax.figure.savefig(path_store_figure+&quot;dicova_track_2_dur&quot;+fmt, bbox_inches=&#39;tight&#39;) plt.show() . . It is nice to see iit_mandi, and iit_jodh (the two new IITs) doing relatively better. | New IITs, iit_hyd, iit_ind, iit_bbs, are following the old ones. | IISc and iit_kan are the poorest in this, and iit_delhi is much better when compared to other old IITs. | . A ratio &gt; 5x is not what an institute of national importance should feature. But sadly, we see this here. Is this due to, . lacking quality applicants | lacking intent in maintaining diversity in recruitment | lacking encouragement to choose a faculty career at these institutes | others | . There is a need to go deeper into analyzing each of these aspects, discuss and address them with dedication and sincerity. . Lets look at the joining dates of Faculty ... . I shift gears now, and look at the joining dates of faculty. . First, we look at the year . #collapse # year of recruitment # dont exclude faculty who have left df = df_fac.copy() x_grid = np.arange(min(df[&#39;joining_year&#39;]),2021) y_grid = np.arange(0,len(institutes)) X, Y = np.meshgrid(x_grid, y_grid) Z_m = np.zeros(X.shape) Z_f = np.zeros(X.shape) for i in range(len(institutes)): for j in range(len(x_grid)): Z_m[i,j] = len(df[(df[&#39;institute&#39;]==institutes[i]) &amp; ((df[&#39;joining_year&#39;]==x_grid[j])) &amp; ((df[&#39;gender&#39;]==&#39;Male&#39;))]) Z_f[i,j] = len(df[(df[&#39;institute&#39;]==institutes[i]) &amp; ((df[&#39;joining_year&#39;]==x_grid[j])) &amp; ((df[&#39;gender&#39;]==&#39;Female&#39;))]) with plt.xkcd(): fig = plt.figure(figsize=[16,10]) FS = 14 ax = plt.subplot(1,1,1) clr = &#39;yellowgreen&#39; ax.bar(x_grid,np.sum(Z_m[:,:],axis=0),alpha=1,width=1.0, color=clr, edgecolor=clr, label=&#39;MALE&#39;) clr = &#39;tab:red&#39; ax.bar(x_grid,np.sum(Z_f[:,:],axis=0),bottom = np.sum(Z_m[:,:],axis=0), alpha=1,width=1.0, color=clr, edgecolor=clr, label=&#39;FEMALE&#39;) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.ylabel(&#39;FACULTY COUNT n (new joinee only)&#39;,fontsize=FS) plt.xlabel(&#39;YEAR&#39;,fontsize=FS) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.legend(frameon=False,loc=&#39;upper left&#39;, fontsize=FS) plt.show() . . Something happened in 1995, and there was a gradual increase in intake | It got reset in 1999, and again began to gradually rise till 2004. | In 2008, with starting of new IITs, there was a rise, with signs of saturation in 2016-2017 | It picked up again in 2018. | Welcome to 2020, the year of pandemic, the count intake reduced but thankfully, did not stop. | . . Breaking this across institutes ... . #collapse # year of recruitment x_grid = np.arange(min(df[&#39;joining_year&#39;]),2021) y_grid = np.arange(0,len(institutes)) X, Y = np.meshgrid(x_grid, y_grid) Z_m = np.zeros(X.shape) Z_f = np.zeros(X.shape) for i in range(len(institutes)): for j in range(len(x_grid)): Z_m[i,j] = len(df[(df[&#39;institute&#39;]==institutes[i]) &amp; ((df[&#39;joining_year&#39;]==x_grid[j])) &amp; ((df[&#39;gender&#39;]==&#39;Male&#39;))]) Z_f[i,j] = len(df[(df[&#39;institute&#39;]==institutes[i]) &amp; ((df[&#39;joining_year&#39;]==x_grid[j])) &amp; ((df[&#39;gender&#39;]==&#39;Female&#39;))]) clr = [&#39;tab:brown&#39;, &#39;tab:pink&#39;,&#39;tab:gray&#39;,&#39;tab:red&#39;,&#39;tab:cyan&#39;, &#39;darkorange&#39;,&#39;wheat&#39;,&#39;yellowgreen&#39;,&#39;cornflowerblue&#39;,&#39;slateblue&#39;, &#39;lightcoral&#39;,&#39;aquamarine&#39;,&#39;tab:purple&#39;] with plt.xkcd(): fig = plt.figure(figsize=[16,12]) FS = 14 ax = plt.subplot(1,1,1) for i in range(0,Z_m.shape[0]): ax.bar(x_grid,Z_m[i,:]+Z_f[i,:],bottom=np.sum(Z_m[:i,:]+Z_f[:i,:],axis=0),alpha=1,width=1.0, color=clr[i], edgecolor=clr[i], label=institutes[i]) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.ylabel(&#39;FACULTY COUNT n (new joinee only)&#39;,fontsize=FS) plt.xlabel(&#39;YEAR&#39;,fontsize=FS) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.legend(frameon=False,loc=&#39;upper left&#39;, fontsize=FS) plt.show() . . Some institutes are always recruiting in good numbers (see iit_mad, and iit_kgp) and some in small numbers (see iisc). | iit_jodh started slow, and recently, has been recruiting good numbers | . . Hows&#39; the gender ratio in joinees ... . #collapse # year of recruitment x_grid = np.arange(min(df[&#39;joining_year&#39;]),2021) y_grid = np.arange(0,len(institutes)) X, Y = np.meshgrid(x_grid, y_grid) Z_m = np.zeros(X.shape) Z_f = np.zeros(X.shape) for i in range(len(institutes)): for j in range(len(x_grid)): Z_m[i,j] = len(df[(df[&#39;institute&#39;]==institutes[i]) &amp; ((df[&#39;joining_year&#39;]==x_grid[j])) &amp; ((df[&#39;gender&#39;]==&#39;Male&#39;))]) Z_f[i,j] = len(df[(df[&#39;institute&#39;]==institutes[i]) &amp; ((df[&#39;joining_year&#39;]==x_grid[j])) &amp; ((df[&#39;gender&#39;]==&#39;Female&#39;))]) with plt.xkcd(): fig = plt.figure(figsize=[12,4]) FS = 14 ax = plt.subplot(1,1,1) clr = &#39;tab:red&#39; ax.plot(x_grid,np.sum(Z_m[:,:],axis=0)/(np.sum(Z_f[:,:],axis=0)+1e-3),marker=&#39;o&#39;,mec=&#39;gray&#39;,ms=10,alpha=.75, color=&#39;tab:red&#39;, label=&#39;ALL&#39;,linewidth=2) ax.plot(x_grid,5*np.ones(x_grid.shape),&#39;--&#39;,alpha=.75,color=&#39;gray&#39;,linewidth=2) # plt.legend(frameon=False, loc=&#39;upper right&#39;, fontsize=FS) plt.ylabel(&#39;MALE-to-FEMALE FACULTY RATIO n (only new joinees)&#39;,fontsize=FS) plt.xlabel(&#39;YEAR&#39;,fontsize=FS) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.set_ylim(0,45) plt.show() . . Before 1993, the ratio was between 10-35, and highly wiggly? | Something happened after 1995, and the ratio became stable between 5-10. | Although, the faculty has increased since 2005 (in most institutes), this ratio has not changed much. | . . Which month do new faculty join? . All of these institutes have a two semester system, namely, July-Dec., and Jan-April. We expect more faculty to join during the start of these semester. This is also seen in the plots at majority of the institutes. But note, faculty also have been joining round the year. . #collapse # month of recruitment x_grid = np.arange(min(df[&#39;joining_month&#39;]),max(df[&#39;joining_month&#39;])+1) y_grid = np.arange(0,len(institutes)) X, Y = np.meshgrid(x_grid, y_grid) Z_m = np.zeros(X.shape) Z_f = np.zeros(X.shape) for i in range(len(institutes)): for j in range(len(x_grid)): Z_m[i,j] = len(df[(df[&#39;institute&#39;]==institutes[i]) &amp; ((df[&#39;joining_month&#39;]==x_grid[j])) &amp; ((df[&#39;gender&#39;]==&#39;Male&#39;))]) Z_f[i,j] = len(df[(df[&#39;institute&#39;]==institutes[i]) &amp; ((df[&#39;joining_month&#39;]==x_grid[j])) &amp; ((df[&#39;gender&#39;]==&#39;Female&#39;))]) with plt.xkcd(): fig = plt.figure(figsize=[16,10]) FS = 10 clr = &#39;tab:cyan&#39; for i in range(Z_m.shape[0]): if i&gt;7: ax = plt.subplot(3,5,i+3) else: ax = plt.subplot(3,5,i+1) ax.bar(x_grid,Z_m[i,:],color=clr,alpha=1,width=0.35,label=&#39;MALE&#39;) ax.bar(x_grid,Z_f[i,:],bottom=Z_m[i,:],color=&#39;tab:red&#39;,alpha=0.35,width=0.5,label=&#39;FEMALE&#39;) plt.legend(frameon=False,loc=&#39;upper left&#39;, fontsize=FS) plt.xticks(x_grid,x_grid,rotation=0, fontsize=FS) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.ylabel(&#39;COUNT&#39;,fontsize=FS) plt.xlabel(&#39;MONTH&#39;,fontsize=FS) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.text(10,max(Z_m[i,:]),institutes[i],fontsize=FS) plt.show() . . and, how about the joining day of the month ... . Is it first of the month? Yes, but again, not always! . #collapse # day of recruitment x_grid = np.arange(min(df[&#39;joining_day&#39;]),max(df[&#39;joining_day&#39;])+1) y_grid = np.arange(0,len(institutes)) X, Y = np.meshgrid(x_grid, y_grid) Z_m = np.zeros(X.shape) Z_f = np.zeros(X.shape) for i in range(len(institutes)): for j in range(len(x_grid)): Z_m[i,j] = len(df[(df[&#39;institute&#39;]==institutes[i]) &amp; ((df[&#39;joining_day&#39;]==x_grid[j])) &amp; ((df[&#39;gender&#39;]==&#39;Male&#39;))]) Z_f[i,j] = len(df[(df[&#39;institute&#39;]==institutes[i]) &amp; ((df[&#39;joining_day&#39;]==x_grid[j])) &amp; ((df[&#39;gender&#39;]==&#39;Female&#39;))]) with plt.xkcd(): fig = plt.figure(figsize=[16,20]) for i in range(Z_m.shape[0]): FS = 10 clr = &#39;tab:cyan&#39; if i&gt;7: ax = plt.subplot(3,5,i+3) else: ax = plt.subplot(3,5,i+1) ax.barh(x_grid,Z_m[i,:],color=clr,alpha=1,height=0.5,label=&#39;MALE&#39;) ax.barh(x_grid,Z_f[i,:],color=&#39;tab:red&#39;,alpha=0.5,height=0.5,label=&#39;FEMALE&#39;) plt.legend(frameon=False,loc=&#39;upper right&#39;) plt.yticks(x_grid,x_grid,rotation=0, fontsize=FS) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.ylabel(&#39;COUNT&#39;,fontsize=FS) plt.xlabel(&#39;DAY OF MONTH&#39;,fontsize=FS) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.text(max(Z_m[i,:])//2,10,institutes[i],fontsize=FS+2) plt.show() . . Months (into job) versus Faculty Designation ... . I had access to the months into job data for each faculty. Below is the plot. . #collapse vals = [] vals.append(df[&#39;months&#39;].values) vals.append(df[&#39;designation&#39;].values) with plt.xkcd(): fig = plt.subplots(figsize=[12,4]) ax = plt.subplot(1,1,1) FS = 12 clr = &#39;cornflowerblue&#39; sns.distplot(df[df[&#39;designation&#39;]==&#39;Assistant&#39;][&#39;months&#39;],label=&#39;Assistant&#39; ,color=&#39;tab:red&#39;) sns.distplot(df[df[&#39;designation&#39;]==&#39;Associate&#39;][&#39;months&#39;],label=&#39;Associate&#39; ,color=&#39;tab:cyan&#39;) sns.distplot(df[df[&#39;designation&#39;]==&#39;Professor&#39;][&#39;months&#39;],label=&#39;Professor&#39; ,color=&#39;cornflowerblue&#39;) ax.legend(frameon=False, loc=&#39;upper right&#39;) ax.vlines(5*12,0,.01,color=&#39;k&#39;,alpha=0.5) ax.vlines(10*12,0,.01,color=&#39;k&#39;,alpha=0.5) ax.vlines(15*12,0,.01,color=&#39;k&#39;,alpha=0.5) ax.text(5*12,0.01,&#39;5 yrs&#39;, fontsize=FS) ax.text(10*12,0.01,&#39;10 yrs&#39;, fontsize=FS) ax.text(15*12,0.01,&#39;15 yrs&#39;, fontsize=FS) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.xlabel(&#39;MONTHS INTO JOB&#39;,fontsize=FS) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . Assitant Professor age during recruitment ... . For faculty aspirants, a key question is when to apply for a faculty job. Although we like to say - &quot;age is just a number&quot;, but when it comes to faculty recruitment, usually the age group of new joinees lies in 30-40 years. Some institutes also advertise preference for below 35 yrs. Since 2015, we can see there is peak around 34 years. . #collapse vals = [] years = [] cnt = 5 for i in range(2015,2021): years.append(i) vals.append(df[(df[&#39;joining_year&#39;]==i) &amp; (df[&#39;designation&#39;]==&#39;Assistant&#39;)][&#39;age&#39;].values-cnt) cnt = cnt-1 with plt.xkcd(): fig = plt.subplots(figsize=[14,6]) FS = 12 clr = &#39;cornflowerblue&#39; for i in range(len(years)): ax = plt.subplot(2,3,i+1) sns.distplot(vals[i],label=str(years[i])) ax.legend(frameon=False, loc=&#39;upper right&#39;) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.xlabel(&#39;AGE&#39;,fontsize=FS) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . What about starting letter of firstnames ... . While I have done basic filtering on names to remove one letter initial, Dr, Prof, SMT, or Mr, etc, still there is a caveat in this observation. All individuals and institutes don&#39;t follow the same norm while putting down their name. For example, there are few instances when surname comes before firstname. But still, lets&#39; see the visulization. . S is a clear winner! | . #collapse data = {} keys = [&#39;name&#39;, &#39;first_char&#39;] for key in keys: data[key] = [] cnt = 0 val_char = [] val_cnt = [] for institute in institutes: val_char.append([]) val_cnt.append([]) data[keys[0]].append(list(df_fac[df_fac[&#39;institute&#39;]==institute][&#39;name&#39;].values)) temp = [] for name in data[keys[0]][cnt]: temp.append(ord(name_filter(name)[0])) data[keys[1]].append(temp) for i in range(10): val_char[cnt].append(chr(max(temp,key=temp.count))) val_cnt[cnt].append(len(np.where(np.array(temp)==ord(val_char[cnt][i]))[0])) temp = remove_items(temp, ord(val_char[cnt][i])) cnt = cnt+1 with plt.xkcd(): fig = plt.subplots(figsize=[12,5]) ax = plt.subplot(1,1,1) clr = [&#39;wheat&#39;,&#39;yellowgreen&#39;,&#39;cornflowerblue&#39;] for i in range(len(institutes)): for j in range(3): ax.bar(j+5*i,val_cnt[i][j],color=clr[j], width=1) ax.text(j+5*i-0.25,val_cnt[i][j],val_char[i][j]) plt.xticks(np.arange(0,61,5)+1,institutes,rotation=45, fontsize=FS) ax.grid(color=&#39;gray&#39;, linestyle=&#39;--&#39;, linewidth=1,alpha=.3) plt.xticks(fontsize=FS) plt.yticks(fontsize=FS) plt.ylabel(&#39;COUNT&#39;,fontsize=FS) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.show() . . Thats&#39; all for now. . A picture can speak a thousand words. It is really nice to see these institutes documenting and sharing their data. I hope this practice continues, and at a larger scale and depth. . An annual graduating PhD Scholar dataset, detailing thesis title, institute, scholars&#39; age, gender, etc., would be great. | If you have suggestion or interpretations or corrections ... feel free to comment below or email me. | Together we can continue making our world a better place! | Please do cross-verify any of your interpretations. | .",
            "url": "https://neerajww.github.io/myblog/2021/06/23/indian_institutes_nirf.html",
            "relUrl": "/2021/06/23/indian_institutes_nirf.html",
            "date": " â€¢ Jun 23, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Curious about Makar Sankranti?",
            "content": "Makar Sankranti (MS) is a Hindu festival, and it is celebrated every year on 14th or 15th January. . Every year - 14th or 15th. Why so? . The dates of most Hindu festivals follow the moon position, and this makes their date vary a lot from one year to another. Makar Sankranti follows the position of Sun (not moon). But does the sun move? Of course, it does. I mean everything in this universe is moving, but - not everything is moving by the same amount. So, do we experience the movement of the Sun? No, at least I don&#39;t, and I think you too don&#39;t and your friend too doesn&#39;t. Then what is this fuss about MS following the position of Sun. . Imagine you are inside a train, and the train is moving and you are sitting still. You took a nap, and the fan fell on you. You woke up, and forgot that the train was moving. Instead, you look out of the window, and start to think the world outside the train is moving and not the train. This is the scenario we will imagine for a while - you are sitting in the train, and the world outside is moving. You noticed 12 stations on the way, but the train didn&#39;t stop on any of them. It is moving, moving and moving, and at the same speed. But wait - what happened to the 13th station! It looks similar to the 1st station. You are surprised. And then comes the 14th station, it looks similar to the 2nd station! Oh my goodness, the stations are repeating. After a while you conclude that the 12 stations keep repeating, and that too in the same order. You have lost your watch, and the train hasn&#39;t stopped since a long time. What do you do? You start keeping track of time via the cycle of the 12 stations! . Now let&#39;s switch to the world we live in. The train is our Earth. It is moving around the Sun on a fixed orbit. Depending on where we are on this orbit, when we look up at the sky we see different constellations. Sometimes we see the Sun entering some constellations, and sometimes leaving some. Humans have been very smart. In our course of years of journey around the Sun, humans have documented that the Sun goes through a cycle of 12 constellations, and this cycle repeats! That&#39;s how we decided to have 12 solar months (my guess). The transition of Sun from one constellation (rashi) to another is called Sankranti. . What is makar though? . Now, what is &quot;Makar&quot; in Makar Sankranti? Makar is a legendary sea-creature in hindu mythology. In hindu astrology, makar refers to a constellation. The Romans call this constellation Capricornus. May be we can say - Hindus and Romans looked at this constellation, and found it to resemble imaginary creatures. The Romans imagined it as a sea-goat! The image below compares the two depictions. . Daylight begins to increase! . According to Hindu astrology, on the day of MS, the Sun enters the Capricornus constellation. This marks the end of short days, and from this day onwards the daylight duration begins to increase. Such an amazing observation, and all this by just being inside the train, or standing on the Earth! Now we have an easy to grasp image of this. The earth (our train analogy) has a tilted axis of rotation, and it revolves around the Sun. Based on where it is in this orbit, the daylight time will change. You would have heard about 21st Dec as the day with the longest night. Sounds a little odd then, right? Yes, that&#39;s according to tropical astrology. The Hindu astrology follows sidereal astrology - the two have some differences, and to know more I will suggest to read about the history of astronomy, I haven&#39;t. According to tropical astrology, the Sun enters Capricornus on 21st Dec and exits on 19 Jan. You can see in the below figure how the duration of &quot;sunrise to sunset&quot; (or daylight time) changes across days of a year. Here, I have plotted it for two Indian cities, and for the year 2019. Lets first consider the eastern city Dibrugarh. We can see that on 21st June the daylight duration is maximum, and then it begins to decrease. On Dec. 21st it reaches a minimum value, and then starts to increase. Two times in a year the daylight equals 12 hrs, these days are referred to as equinox. We can see a similar pattern for the Western city Porbandar. . And what&#39;s the reason for celebration? . Further, as the occurrence of Makar Sankranti relates to the increase in amount of sunlight received by earth in the coming days, many life forms also respond to this event. In India, you will find migratory birds start to return to hills, and winter is about to end with spring season coming soon. Rice is harvested in December, and sugarcane in December-March. Hence, although MS has astrological relevance, it indirectly also marks a joyous moment for many life forms, including humans - at least when agriculture was the major occupation! . That&#39;s it . This post is my limited knowledge about Makar Sankranti. In case you have some suggestions, feel free to use the comment section below. . | If you are more interested, I encourage you to see the wikipedia page here. It has a wealth of information about how Makar Sankranti is celebrated in different cultures across India.Â  . | .",
            "url": "https://neerajww.github.io/myblog/2021/01/14/makar_sankranti.html",
            "relUrl": "/2021/01/14/makar_sankranti.html",
            "date": " â€¢ Jan 14, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Crawling tweets",
            "content": "About . We will write few lines of python codes to download the tweets from a twitter handle. Note that, you can open this webpage as a ipython notebook by clocking the colab tab shown on the top right panel (up). . import GetOldTweets3 as got import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1 import make_axes_locatable # to move placement of colorbar from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) . username = &#39;MoHFW_INDIA&#39; # init a twitter hander count = 10000 # how many recent tweets to download tweetCriteria = got.manager.TweetCriteria().setUsername(username) .setSince(&quot;2015-06-01&quot;) .setUntil(&quot;2020-06-29&quot;) .setMaxTweets(count) # download tweets tweets = got.manager.TweetManager.getTweets(tweetCriteria)# Creating list of chosen tweet data # segregate tweet date and text tweets_date = [tweet.date for tweet in tweets] tweets_text = [tweet.text for tweet in tweets] # segregate tweet by date and count per date tweets_cal_1 = np.zeros((31,12)) tweets_cal_2 = np.zeros((31,12)) tweets_date_1 = [] tweets_date_2 = [] for i in range(len(tweets_text)): if tweets_date[i].year == 2019: tweets_cal_1[tweets_date[i].day-1,tweets_date[i].month-1]+= 1 tweets_date_1.append(str(tweets_date[i].month)+&#39;-&#39;+str(tweets_date[i].year)) if tweets_date[i].year == 2020: tweets_cal_2[tweets_date[i].day-1,tweets_date[i].month-1]+= 1 tweets_date_2.append(str(tweets_date[i].month)+&#39;-&#39;+str(tweets_date[i].year)) # stack to a single np array tweets_cal = np.hstack((tweets_cal_1,tweets_cal_2)) . # visualiza the tweet count distrubted over DAY-MONTH-YEAR fig,ax = plt.subplots(1,1,figsize=(12,5)) X, Y = np.arange(1,tweets_cal.shape[0]+2), np.arange(0,tweets_cal.shape[1]) im = ax.pcolormesh(Y,X, tweets_cal, vmin=0, vmax=np.max(tweets_cal), cmap=&#39;Blues&#39;) plt.xlim(0,18) plt.xticks(np.arange(0,18)+.5, xticks,rotation=30) cbar = fig.colorbar(im, ax=ax) cbar.set_label(&#39;NO. OF TWEETS&#39;,size=13) plt.ylabel(&quot;DAY OF MONTH&quot;,fontsize=13) plt.xlabel(&quot;MONTH OF YEAR&quot;,fontsize=13) plt.title(&#39;Twitter handle: @&#39;+username,fontsize=13) plt.show() fig, ax = plt.subplots(1,1,figsize=(10,4)) ax.plot(np.sum(tweets_cal,axis=0)) ax.set_xlabel(&quot;MONTH OF YEAR&quot;,fontsize=13) ax.set_ylabel(&#39;NO. OF TWEETS&#39;,fontsize=13) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.tick_params(which=&#39;both&#39;, width=2) ax.grid(True) plt.xticks(np.arange(0,18), xticks,rotation=30) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # ax.plot([5,60],[5,60],&#39;--&#39;,color=&#39;black&#39;,alpha=0.25) ax.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) plt.xlim(0,17) plt.show() . Observations . The twitter handle used above belongs to the Minstry of Health, Government of India. We can see a spur in the tweet count from 3-20 (March 2020) onwards. This month onwards the COVID-19 awareness grew in India. . Next . We will try doing some basic NLP on the tweet texts. .",
            "url": "https://neerajww.github.io/myblog/2020/06/29/crawl_tweets.html",
            "relUrl": "/2020/06/29/crawl_tweets.html",
            "date": " â€¢ Jun 29, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Sounds of languages",
            "content": "About . Being labelled as social animals, communicating our thoughts plays a critical role for a healthy life. Spoken language communication is a skill which humans have evolved to use. In this post I intend to touch upon some questions. Feel free to use the comments option below to suggest (or recommend) any resources to dig further. . I read the article - Speech Acoustics of the Worldâ€™s Languages by Tucker and Wright, in the Acoustic Today magazine. By the way, this is a freely available magazine, often with nice articles. Coming back to summarizing the article I read, the key highlights included: . Currently, there are more than 7000 spoken languages | Acoustic signal procesing research published in international journals has focussed mostly on English. | Lot remains unquestioned (and hence, also unanswered) about scientific understanding of speech production and perception of most of the languages. | Same holds when it comes to technology applications, such as automatic speech recognition and text-to-speech conversion. Most technology is designed for English. | . A popular speech production model is the source-filter model. It assumes the sound originates at a source (e.g. vibration of vocal folds or vocal chords, lying horizontally in the larynx). As the air pressure travels through the vocal tract, the sound gets modified or filtered. The source-filter model when applied to different spoken sounds reveals insight into the physical configurations of the source and filter used during speaking different sounds. These insights have helped understand aspects such as pitch (or the fundamental frequency) and formant frequencies of the speaker. . The smallest unit of spoken speech sound is referred as phoneme. These include vowels and consonants. Every spoken language has a phoneme set. The phoneme set of two language can overlap. Does a language have more consonants or vowels? Let&#39;s see this by pulling and visualizing some data from Phoible. You can scroll the below code and jump to the plot. . # import some packages import pandas as pd import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.axes_grid1 import make_axes_locatable # to move placement of colorbar from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) import seaborn as sns sns.set() # Use seaborn&#39;s default style to make attractive graphs sns.set_style(&quot;white&quot;) sns.set_style(&quot;ticks&quot;) ############# load csv file df = pd.read_csv(&#39;./my_data/vowel_consonant_languages.csv&#39;) ############ plot data fig = plt.subplots(figsize=(6,6)) ax = plt.subplot(1,1,1) ax.scatter(df[&#39;count_consonant&#39;],df[&#39;count_vowel&#39;],color=&#39;black&#39;,alpha=.5) ax.set_xlabel(&#39;CONSONANT COUNT&#39;,fontsize=13) ax.set_ylabel(&#39;VOWEL COUNT&#39;,fontsize=13) # A.U stands for Arbitrary Units ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.tick_params(which=&#39;both&#39;, width=2) ax.tick_params(which=&#39;major&#39;, length=7) ax.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.xlim([0,80]) plt.ylim([0,80]) fig = plt.subplots(figsize=(10,5)) ax = plt.subplot(1,1,1) sns.distplot(df[&#39;count_vowel&#39;],color=&#39;red&#39;,label=&#39;VOWEL&#39;) sns.distplot(df[&#39;count_consonant&#39;],color=&#39;blue&#39;,label=&#39;CONSONANT&#39;) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.set_xlabel(&#39;COUNT ACROSS LANGUAGES&#39;,fontsize=13) ax.set_ylabel(&#39;DENSITY&#39;,fontsize=13) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=13) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.show() . len(df[&#39;count_consonant&#39;]) . 2000 . In the above plots, the left panel shows that all the 2000 datapoints (each denotes a language) have a higher count for consonant compared to vowels. The right panel depicts a histogram of the vowel and consonant counts across 2000 languages. The vowel count peaks at around 10 and the consonant count peaks at around 23. Also, the consonant count has more spread. Here is my first question. . Why are there more consonants than vowels in most spoken languages? Is there a theory to explain this. Is it more to do with limitations on speech production or perception abilities. . Phonotactics refers to how the phonemes are combined in the spoken language. Two languages can have the same phoneme set but can differ significantly in phonotactics. You cannot randomly combine phonemes and make an intelligibile speech sound. Every spoken language would have gone through a stage of evolution to come up with its own stable phonetactics. Here is my second question. . Given the phoneme set of two languages, is it possible to meaningfully quantify the similarity between the languages. I think, the answer has to also use the phonetactics. If yes, can we make a plot of how close is Hindi to other 8000 spoken languages. May be Duolingo will know more about this. . How are the origins of languages distributed on earth surface? I tried visualizing this by pulling data from the interesting Glottolog dataset. Here&#39;s what I did: . Downloaded the languages+geo location CSV data | Installed Basemap python package to overlay data on maps | Wrote few lines of code to make the visualization | . Let&#39;s see the visualization of how 8125 languages are distributed on surface of earth. You can scroll the code and directly jump to the plot below. . from mpl_toolkits.basemap import Basemap df = pd.read_csv(&#39;./my_data/languages_and_dialects_geo.csv&#39;) df_new = df.dropna(subset=[&#39;latitude&#39;, &#39;longitude&#39;]) df_new = df_new[df_new[&#39;level&#39;]==&#39;language&#39;].reset_index(drop=True) macroarea = df_new[&#39;macroarea&#39;].unique() indx = [] for i in range(len(macroarea)): indx.append(df_new[df_new[&#39;macroarea&#39;]==macroarea[i]].index) # Extract the data we&#39;re interested in lat = df_new[&#39;latitude&#39;].values lon = df_new[&#39;longitude&#39;].values fig = plt.figure(figsize=(16, 8)) m = Basemap() m.drawcoastlines() clr = [&#39;tab:green&#39;,&#39;blue&#39;,&#39;tab:red&#39;,&#39;magenta&#39;,&#39;black&#39;,&#39;darkred&#39;] for i in range(len(macroarea)): m.scatter(lon[indx[i]], lat[indx[i]], latlon=True,alpha=0.2,color=clr[0]) . In the above plot you can spot ... . most languages are distributed close to the equator. This might be because these places were more densely populated 1500-2000 years ago. | . Interestingly, in 2006 Papua New Guinea had 832 living languages, making it the most linguistically diverse place on Earth. My third question is ... . What necessitates a new language creation? . and my fourth question is ... . Can we cluster languages based on their acoustics? How will the cluster relate to geographical closeness of their origins. . That&#39;s it as of now. . Extras . From the aspect of language perception by humans, I came across The Great Language Game, a game were a user is asked to identify the spoken language by listening to an audio snippet. This fun project went on for few years, and the results are published here. The results are worth a look. I will add my summary after going though it. . To know more about language diversity on our Earth, you may find this paper useful. | Installing basemap in MAC: brew install geos pip3 install https://github.com/matplotlib/basemap/archive/master.zip . | .",
            "url": "https://neerajww.github.io/myblog/2020/06/28/language_spread.html",
            "relUrl": "/2020/06/28/language_spread.html",
            "date": " â€¢ Jun 28, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "A short tale on the longest day",
            "content": "About . I had forgotten about the longest day. But now since 2017, around 21st June, I always notice in social media some news about it. Now, to further strengthen my excitement, I have my nephew who shares the birthday with summer solstice, and also two wonderful friends whose birthdays are one day before and one day after this day. . This time I read an exciting story shared by Vishu Guttal here. He describes his visit to a school and an exercise he did school students on verifying the &quot;longest day&quot; fact. I was amazed on reading it, and suggest you too! Subsequently, I thought of doing the exercise myself. This post is a result of that. . Astronomy is amazing. You can make theory, prediction, observations, and verification - but all this without ever getting close to the entity you are studying. This science has amazed humanity for centuries - Does earth go around the Sun? Is the orbit elliptical? Are there many galaxies? Is the universe expanding? Did all this start with a big bang? - Theories have been made, verified, approved, disapproved, and updated. Thatâ€™s the definition of science. . Let&#39;s take a very small ride into this field as we sit (or stand, whatever you are doing) and verify - the &quot;longest day&quot; fact by crunching some numbers and plotting the data. . First let&#39;s list a few other known facts. . Earth goes around Sun | This path is elliptic | Earth is tilted about its axis | . It is not easy to verify these three bullets. Spare a moment, and imagine staring at the sky, and verifying the above statements. It is not easy, and you will thank some amazing folks who did this. As a result of these facts we now understand why we experience on Earth: . seasons | a longest day and a shortest day | . Intuitively, the season should be determined by length of the day. A longer day will imply more heat incident on the Earth surface, indicating a day in summer. The below figure from Wikipedia helps understanding this. The interesting thing to note is that summer is not when earth is closest to the Sun. This is because of the tilt of the Earth. Owing to this tilt we have a day of the year on which the northern hemisphere is exposed to the Sun for maximum time, while the Earth rotates around its own axis. This day is called the summer solstice. Similarly, we have a day of winter solstice - shortest day. . So why did the Earth tilt? One theory states - Long long time ago something came flying by and hit Earth, and since then, our Earth got tilted! To read more click here. The nice thing is thus, we have seasons! . . Can you think of how we can verify the longest day claim. Spare a few moments. In the code below we will do a small exercise to verify that there exists a longest day. We will do this for just one year, 2019. The same thing can be done for any year if you are in doubt. The idea is simple. . Using python we will write few lines of codes | This will use &quot;astral package&quot; to find the sunrise/sunset times for any latitude (and longitude) on Earth. | We will visualize this data | . Fingers crossed on what the plots will look like, let&#39;s keep scrolling. . Let&#39;s code this . #collapse # import some packages import pandas as pd import numpy as np import datetime import matplotlib.pyplot as plt import time from astral import LocationInfo from astral import sun import pytz from mpl_toolkits.axes_grid1 import make_axes_locatable # to move placement of colorbar from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) . . ImportError Traceback (most recent call last) &lt;ipython-input-1-c60cf7950d23&gt; in &lt;module&gt;() 6 import matplotlib.pyplot as plt 7 import time -&gt; 8 from astral import LocationInfo 9 from astral import sun 10 import pytz ImportError: No module named astral . Step 1: We will load a CSV file which contains the latitude and longitude location of 212 cities in India. This is good as we can visualize the sunrise/sunset times across India, particularly, from East to West. . #collapse # load indian cities dataset df = pd.read_csv(&#39;./my_data/indian_cities_lat_long.csv&#39;) # sort rows from east to west (that is, longitude values) df = df.sort_values(&#39;lng&#39;,ascending=False) df = df.reset_index(drop=True) . . Step 2: We will make a variable containing all dates of 2019 . #collapse # get all dates in 2019 start = datetime.datetime(2019, 1, 1, 0, 0, 0) end = datetime.datetime(2019, 12, 31, 0, 0, 0) delta = end - start . . Step 3: We will call the astral package and compute the sunset/sunrise time for all 212 cities for all 365 days of 2019. . #collapse data = {} data[&#39;sunrise&#39;] = [] data[&#39;sunset&#39;] = [] data[&#39;noon&#39;] = [] for cnt in range(len(df)): data[&#39;sunrise&#39;].append([]) data[&#39;sunset&#39;].append([]) data[&#39;noon&#39;].append([]) i = 0 for day in range(delta.days + 1): t_start = time.time() this_date = str((start+datetime.timedelta(days=day)).date()) params = {&#39;lat&#39;:df[&#39;lat&#39;][cnt],&#39;lng&#39;:df[&#39;lng&#39;][cnt],&#39;date&#39;:this_date} tz = pytz.timezone(&#39;Asia/Kolkata&#39;) l = LocationInfo() l.name = &#39;name&#39; l.region = &#39;region&#39; l.latitude = df[&#39;lat&#39;][cnt] l.longitude = df[&#39;lng&#39;][cnt] s = sun.sun(l.observer, date=start+datetime.timedelta(days=day),tzinfo=tz) data[&#39;sunrise&#39;][cnt].append(s[&quot;sunrise&quot;].time().strftime(&#39;%H:%M:%S&#39;)) data[&#39;sunset&#39;][cnt].append(s[&quot;sunset&quot;].time().strftime(&#39;%H:%M:%S&#39;)) data[&#39;noon&#39;][cnt].append(s[&quot;noon&quot;].time().strftime(&#39;%H:%M:%S&#39;)) . . Step 4: Some data structuring into numpy arrays to ease later visualization. . #collapse sun_rise_in_secs = [] sun_set_in_secs = [] sun_overhead_in_secs = [] sun_length_in_secs = [] for i in range(len(data[&#39;sunrise&#39;])): sun_rise_in_secs.append([]) sun_set_in_secs.append([]) sun_overhead_in_secs.append([]) sun_length_in_secs.append([]) for j in range(len(data[&#39;sunrise&#39;][0])): time_1 = data[&#39;sunrise&#39;][i][j].split(&#39;:&#39;) time_2 = data[&#39;sunset&#39;][i][j].split(&#39;:&#39;) time_3 = data[&#39;noon&#39;][i][j].split(&#39;:&#39;) sun_rise_in_secs[i].append(float(time_1[0])*60*60+float(time_1[1])*60+float(time_1[2])) sun_set_in_secs[i].append(float(time_2[0])*60*60+float(time_2[1])*60+float(time_2[2])) sun_overhead_in_secs[i].append(float(time_3[0])*60*60+float(time_3[1])*60+float(time_3[2])) sun_length_in_secs[i].append(sun_set_in_secs[i][j]-sun_rise_in_secs[i][j]) sun_rise_in_secs = np.array(sun_rise_in_secs) sun_set_in_secs = np.array(sun_set_in_secs) sun_overhead_in_secs = np.array(sun_overhead_in_secs) sun_length_in_secs = np.array(sun_length_in_secs) # np.argmax(daylength_in_secs,axis=1) . . Done. Now let&#39;s visualize . First lets visualize the day length, that is the difference between sunset and sunrise times, for all the 365 days. . #collapse # plot daylength day_max = np.argmax(sun_length_in_secs,axis=1)+1 # date_max = [] # for i in range(len(day_max)): # date_max.append(datetime.datetime(2019, 1, 1) + datetime.timedelta(day_max[i] - 1)) fig = plt.subplots(figsize=(16,7)) ax = [] ax.append(plt.subplot(1,1,1)) ax[0].plot([14,14],[9,15],&#39;--&#39;,color=&#39;blue&#39;,linewidth=2,alpha=0.8) ax[0].plot([171,171],[9,15],&#39;--&#39;,color=&#39;blue&#39;,linewidth=2,alpha=0.8) ax[0].plot([355,355],[9,15],&#39;--&#39;,color=&#39;blue&#39;,linewidth=2,alpha=0.8) ax[0].plot([0,365],[12,12],&#39;--&#39;,color=&#39;green&#39;,linewidth=2,alpha=.8) # ax[0].plot(sun_length_in_secs.T/60/60,alpha=0.4) ax[0].plot(sun_length_in_secs[0]/60/60,color=&#39;tab:red&#39;,linewidth=5,label=df[&#39;city&#39;][0]) ax[0].plot(sun_length_in_secs[-1]/60/60,color=&#39;tab:blue&#39;,linewidth=5,label=df[&#39;city&#39;][len(df)-1]) ax[0].text(2,13,&#39;15th Jan&#39;,rotation=90,fontsize=14) ax[0].text(158,13,&#39;21st June&#39;,rotation=90,fontsize=14) ax[0].text(342,13,&#39;21st Dec&#39;,rotation=90,fontsize=14) ax[0].set_xlabel(&quot;DAYS SINCE 1st JAN&quot;,fontsize=14) ax[0].set_ylabel(&quot;DAYLIGHT DURATION [in hrs]&quot;,fontsize=14) ax[0].grid(True) ax[0].spines[&#39;right&#39;].set_visible(False) ax[0].spines[&#39;top&#39;].set_visible(False) ax[0].xaxis.set_minor_locator(AutoMinorLocator()) ax[0].yaxis.set_minor_locator(AutoMinorLocator()) ax[0].tick_params(which=&#39;both&#39;, width=2) ax[0].tick_params(which=&#39;major&#39;, length=7) ax[0].tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax[0].legend(frameon=False, fontsize=14) # im = ax[1].imshow(sun_length_in_secs/60/60, cmap=&#39;RdBu_r&#39;) # ax[1].set_xlabel(&quot;DAYS SINCE 1st JAN&quot;,fontsize=14) # ax[1].set_ylabel(&quot;CITIES&quot;,fontsize=14) # divider = make_axes_locatable(ax[1]) # colorbar_ax = fig.add_axes([.92, 0.2, 0.01, 0.5]) # cbar = fig.colorbar(im, cax=colorbar_ax) # cbar.set_label(&#39;DAY LENGTH [in hrs]&#39;,size=13) # ax[1].plot(day_max-1,np.arange(0,sun_length_in_secs.shape[0],1),&#39;o-&#39;,color=&#39;k&#39;) # yticks = [0,50,100,150,200] # keys = [] # for i in range(len(yticks)): # keys.append(df[&#39;city&#39;][yticks[i]]) # ax[1].set_yticks(yticks) # ax[1].set_yticklabels(keys,rotation=0,fontsize=13) plt.show() . . You can see that there is a nice peak around 21st June for the day length data. This peak is there in traces for all the 212 cities, and the patterns is similar. For the majority of the cities this happens to be exactly 21st June. Here, Dibrugarh is the eastern most city in our database, and Porbandar is the westernmost. The interesting thing is also the variation in the trace of day length across the cities. Also, note that there are two days of the year on which day length is exactly 12 hrs! These dates correspond to the equinox. . Next, let&#39;s visualize the sunrise times for all 365 days. The code is given below. . #collapse fig, ax = plt.subplots(1,2,figsize=(16,7)) ax[0].plot(sun_rise_in_secs.T/60/60,alpha=0.4) ax[0].plot(sun_rise_in_secs[0]/60/60,color=&#39;k&#39;,linewidth=5,label=df[&#39;city&#39;][0]) ax[0].plot(sun_rise_in_secs[-1]/60/60,color=&#39;r&#39;,linewidth=5,label=df[&#39;city&#39;][len(df)-1]) ax[0].set_xlabel(&quot;DAYS SINCE 1st JAN&quot;,fontsize=14) ax[0].set_ylabel(&quot;SUNRISE [in hrs]&quot;,fontsize=14) ax[0].grid(True) ax[0].spines[&#39;right&#39;].set_visible(False) ax[0].spines[&#39;top&#39;].set_visible(False) ax[0].xaxis.set_minor_locator(AutoMinorLocator()) ax[0].yaxis.set_minor_locator(AutoMinorLocator()) ax[0].tick_params(which=&#39;both&#39;, width=2) ax[0].tick_params(which=&#39;major&#39;, length=7) ax[0].tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax[0].legend() im = ax[1].imshow(sun_rise_in_secs/60/60, cmap=&#39;RdBu_r&#39;) ax[1].set_xlabel(&quot;DAYS SINCE 1st JAN&quot;,fontsize=14) ax[1].set_ylabel(&quot;CITIES&quot;,fontsize=14) divider = make_axes_locatable(ax[1]) colorbar_ax = fig.add_axes([.92, 0.2, 0.01, 0.5]) cbar = fig.colorbar(im, cax=colorbar_ax) cbar.set_label(&#39;SUNRISE [in hrs]&#39;,size=13) yticks = [0,50,100,150,200] keys = [] for i in range(len(yticks)): keys.append(df[&#39;city&#39;][yticks[i]]) ax[1].set_yticks(yticks) ax[1].set_yticklabels(keys,rotation=0,fontsize=13) plt.show() . . In the above plots you can see a nice variation in sunrise times across the year. Further, the eastern most city sees an early sunrise compared to the western most. Also, sunrises early in summer and late in winter! . Next, let&#39;s visualize the sunset times. The code snippet is given below. . #collapse fig, ax = plt.subplots(1,2,figsize=(16,7)) ax[0].plot(sun_set_in_secs.T/60/60,alpha=0.4) ax[0].plot(sun_set_in_secs[0]/60/60,color=&#39;k&#39;,linewidth=5,label=df[&#39;city&#39;][0]) ax[0].plot(sun_set_in_secs[-1]/60/60,color=&#39;r&#39;,linewidth=5,label=df[&#39;city&#39;][len(df)-1]) ax[0].set_xlabel(&quot;DAYS SINCE 1st JAN&quot;,fontsize=14) ax[0].set_ylabel(&quot;SUNSET TIME [in hrs]&quot;,fontsize=14) ax[0].grid(True) ax[0].spines[&#39;right&#39;].set_visible(False) ax[0].spines[&#39;top&#39;].set_visible(False) ax[0].xaxis.set_minor_locator(AutoMinorLocator()) ax[0].yaxis.set_minor_locator(AutoMinorLocator()) ax[0].tick_params(which=&#39;both&#39;, width=2) ax[0].tick_params(which=&#39;major&#39;, length=7) ax[0].tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax[0].legend() im = ax[1].imshow(sun_set_in_secs/60/60, cmap=&#39;RdBu_r&#39;) ax[1].set_xlabel(&quot;DAYS SINCE 1st JAN&quot;,fontsize=14) ax[1].set_ylabel(&quot;CITIES&quot;,fontsize=14) divider = make_axes_locatable(ax[1]) colorbar_ax = fig.add_axes([.92, 0.2, 0.01, 0.5]) cbar = fig.colorbar(im, cax=colorbar_ax) cbar.set_label(&#39;SUNSET [in hrs]&#39;,size=13) yticks = [0,50,100,150,200] keys = [] for i in range(len(yticks)): keys.append(df[&#39;city&#39;][yticks[i]]) ax[1].set_yticks(yticks) ax[1].set_yticklabels(keys,rotation=0,fontsize=13) plt.show() . . Here again we see a nice pattern composed of a peak and a trough. . Next, lets plot the time of noon, or the time when the sun is at its highest point directly above the observer. Below is the code snippet to plot it. . #collapse fig, ax = plt.subplots(1,2,figsize=(16,7)) ax[0].plot(sun_overhead_in_secs.T/60/60,alpha=0.4) ax[0].plot(sun_overhead_in_secs[0]/60/60,color=&#39;k&#39;,linewidth=5,label=df[&#39;city&#39;][0]) ax[0].plot(sun_overhead_in_secs[-1]/60/60,color=&#39;r&#39;,linewidth=5,label=df[&#39;city&#39;][len(df)-1]) ax[0].set_xlabel(&quot;DAYS SINCE 1st JAN&quot;,fontsize=14) ax[0].set_ylabel(&quot;NOON [in hrs]&quot;,fontsize=14) ax[0].grid(True) ax[0].spines[&#39;right&#39;].set_visible(False) ax[0].spines[&#39;top&#39;].set_visible(False) ax[0].xaxis.set_minor_locator(AutoMinorLocator()) ax[0].yaxis.set_minor_locator(AutoMinorLocator()) ax[0].tick_params(which=&#39;both&#39;, width=2) ax[0].tick_params(which=&#39;major&#39;, length=7) ax[0].tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax[0].legend() im = ax[1].imshow(sun_overhead_in_secs/60/60, cmap=&#39;RdBu_r&#39;) ax[1].set_xlabel(&quot;DAYS SINCE 1st JAN&quot;,fontsize=14) ax[1].set_ylabel(&quot;CITIES&quot;,fontsize=14) divider = make_axes_locatable(ax[1]) colorbar_ax = fig.add_axes([.92, 0.2, 0.01, 0.5]) cbar = fig.colorbar(im, cax=colorbar_ax) cbar.set_label(&#39;NOON [in hrs]&#39;,size=13) yticks = [0,50,100,150,200] keys = [] for i in range(len(yticks)): keys.append(df[&#39;city&#39;][yticks[i]]) ax[1].set_yticks(yticks) ax[1].set_yticklabels(keys,rotation=0,fontsize=13) plt.show() . . Here we see something which I didn&#39;t expect. India has one timezone and hence, we see a gradual increase in noon time from east to west. But what are those sinusoidal patterns, and also there is a downward moving trend. . That&#39;s it . We verified that there does exist a longest day around 21st June. For the majority of the cities this day was 21st June, and for a few others it was 20 or 22 June. Also, we did not verify here, but it appears that around 21st June the day lengths differ by a few seconds. . | Twice in a year the day length is equal to the night length. . | The noon time varies across the year in a pattern which does seem interesting. . | . On top of this, I also find it interesting that we can estimate the sunrise/sunset/noon times using some math equations. There is a wiki article on this. I will try to know more about this sometime. As of now, I would like to thank the astral package for the implementation. I tried first using a free API but the query to get the data was taking time in hrs. I will also like to than Vishu Guttal for sharing the nice initiative and also publishing it in Resonance, a Science Communication journal. .",
            "url": "https://neerajww.github.io/myblog/2020/06/22/summer_solstice.html",
            "relUrl": "/2020/06/22/summer_solstice.html",
            "date": " â€¢ Jun 22, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Escaping Praat scripting using python",
            "content": "What is Praat . Praat is a computer programme to analyze speech signals. The software interface provides a GUI allowing insightful visualization of features, such as pitch, formant, intensity, spectrogram, etc. On top of this, it also has a feature to write script enabling analysis of a pile of speech files. So, in short, it is a popular and useful research tool for speech analysis. But I haven&#39;t found the scripting language of Praat easy to understand. Actually, I have been scared to read the praat scripts also because the syntax looks alien to me. This is not to scare you as there are many people using it beautifully. . Parselmouth comes to rescue . Then one day I came across parselmouth, and this has given me some peace! Quating from its website: . Parselmouth is unique in its aim to provide a complete and Pythonic interface to the internal Praat code. While other projects either wrap Praatâ€™s scripting language or reimplementing parts of Praatâ€™s functionality in Python, Parselmouth directly accesses Praatâ€™s C/C++ code (which means the algorithms and their output are exactly the same as in Praat) and provides efficient access to the programâ€™s data, but also provides an interface that looks no different from any other Python library. . In this notebook I will provide some code snippets to analyze a speech file using parselmouth (and thus, praat). . Example . We will read a WAV file and extract some features. I must say that the package is still under development. You may not find all the features of praat implemented yet. But at the current stage itself I am finding it useful. . #collapse import numpy as np import parselmouth import matplotlib.pyplot as plt [fs, x] = wavfile.read(&#39;./my_sounds/count.wav&#39;) x = x/np.max(np.abs(x)) t = np.arange(0,len(x))/fs # for PRAAT sr = 16000 hop_dur = .01 num_form = 3 max_form_freq = 4500 # call parselmouth to load sound snd = parselmouth.Sound(&#39;./my_sounds/count.wav&#39;) # extract features pitch = snd.to_pitch(time_step=hop_dur) # pitch track harm = snd.to_harmonicity(time_step=hop_dur) # harmonic-to-noise ratio form = snd.to_formant_burg(time_step=hop_dur,max_number_of_formants=num_form, maximum_formant = max_form_freq, window_length=win_dur, pre_emphasis_from=50.0) # formants intensity = snd.to_intensity(minimum_pitch = 75.0, time_step=hop_dur,subtract_mean=False) # intensity spectrogram = snd.to_spectrogram(window_length=0.04) times = pitch.ts() # analysis window time instants pitch_vals = [] harm_vals = [] form_1_vals = [] form_2_vals = [] form_3_vals = [] inten_vals = [] for dt in times: pitch_vals.append(pitch.get_value_at_time(dt)) harm_vals.append(harm.get_value(dt)) form_1_vals.append(form.get_value_at_time(1,dt)) form_2_vals.append(form.get_value_at_time(2,dt)) form_3_vals.append(form.get_value_at_time(3,dt)) inten_vals.append(intensity.get_value(dt)) . . /Users/neeks/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:5: WavFileWarning: Chunk (non-data) not understood, skipping it. &#34;&#34;&#34; . Lets visualize some of the features . fig = plt.subplots(figsize=(12,8)) ax = plt.subplot(2,2,1) ax.plot(times,pitch_vals) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;PITCH FREQ [in Hz]&#39;) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.xlim(times[0],times[-1]) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(2,2,2) ax.plot(times,inten_vals) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;INTENSITY [in dB]&#39;) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.xlim(times[0],times[-1]) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax = plt.subplot(2,2,3) ax.plot(times,harm_vals) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;HARMONIC-TO-NOISE RATIO [in dB]&#39;) plt.xlim(times[0],times[-1]) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.ylim(0,50) ax = plt.subplot(2,2,4) ax.plot(times,np.array(form_1_vals)/1e3) ax.plot(times,np.array(form_2_vals)/1e3) ax.plot(times,np.array(form_3_vals)/1e3) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;FORMANT FREQ [in kHz]&#39;) plt.xlim(times[0],times[-1]) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) fig,ax = plt.subplots(1,1,figsize=(12,5)) dynamic_range = 70 X, Y = spectrogram.x_grid(), spectrogram.y_grid() sg_db = 10 * np.log10(spectrogram.values) im = ax.pcolormesh(X, Y, sg_db, vmin=sg_db.max() - dynamic_range, cmap=&#39;RdBu_r&#39;) fig.colorbar(im, ax=ax) plt.ylim([spectrogram.ymin, spectrogram.ymax]) plt.xlabel(&quot;TIME [in s]&quot;) plt.ylabel(&quot;FREQUENCY [in Hz]&quot;) . Text(0, 0.5, &#39;FREQUENCY [in Hz]&#39;) . Lets also interpret these features. . Pitch is a perceived acoustic feature which relates to the vibratory frequency of the vocal folds. | Intensity of the perceived correlate of loudness of speech | Harmonic-to-noise ratio relates to the quality of the speech, example, hoarse (low HNR) | Formant frequencies quantify the resonance frequecies of the vocal tract while we speak. You can read more about these features in the context of Praat at its website. | . Coming to the above plots, we can see that our speaker usually has a pitch close to 140 Hz. So we can bet the speaker is a male. The speaker is loud enough, and the HNR around 15 dB indicates a good quality speech. All these obaservations get further strengthened on seeing the narrowband spectrogram. . That&#39;s it . On top of these features you can also extract MFCCs, spectrogram, etc., do batch processing of files etc. You can learn about Parcelmouth usage here. Also, there is lot more here as well. . I hope you find this useful. .",
            "url": "https://neerajww.github.io/myblog/2020/06/16/PRAAT_feature_extractiion.html",
            "relUrl": "/2020/06/16/PRAAT_feature_extractiion.html",
            "date": " â€¢ Jun 16, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Polynomial regression using Gradient Descent",
            "content": "What is polynomial regression . Our friend Timmy gave us some data composed of input into a system and the corresponding output from the system. We will refer to the input as the independent variable (x) and the output as the dependent variable (y). We plot the data on an x versus y scatter plot. . The plot on the left looks neat. We know some high school mathematics and so start to wonder can we fit a polynomial to this data. That is, can we write y as follows: . $$y = sum_{p=1}^{P}a_{p}x^{p}+w$$ . where P is the order of the polynomial, a_p are the polynomial coefficients, and w is what could not be modeled by the polynomial (or noise). Examples of these fits are shown in the right plot. But why do we fit anything to the data given by Timmy? The simple answer is - we are curious and we know little math, and there are good tools at are service to do this. Remember, F. Gauss, the famous, also did this when he collected astronomy data, and his tools were much more laborious (pen and paper). Coming back, this curiosity may lead us to identify some relationship between input and output. What happens when you discover a relationship? The output does not looks random anymore, and given an input you can predict something close to what the system will output. Depending on the data you are trying to model (or Timmy gave), this can be amazing. It can help you understand the system or predict outputs and devise actions. . How is it done . Lets ask - How can when say the polynomial is a bad fit? In short, we have to first quantify what is good or bad here. Given an input x, we obtain the difference between the true output and polynomial model output. This is the error. The sign of the error is not a concern so we square this error. The squaring also make the error (as a function of input, a_p) a differentiable function. We sum this error over all N samples. That is, . $$E = sum_{n=1}^{N}(y[n]-y_{hat} [n])^2$$ $$y_{hat}[n]= sum_{p=1}^{P}a_{p}x^{p}[n]$$ . We will say the fit is good if the error is low. Again, low is qualitative, lower the better. But how much low is okay? Well, this depends on the application. . How will we do it here - Gradient Descent . We will use high school calculus, specifically, derivative. The derivative of a curve at any point gives the slope at that point. This is useful in many ways and particularly for our error function E, introduced above. The error function E is quadratic in a_p, and that means it is convex and reaches a minimum at a specific point in the multi-dimensional plane defined by a_ps. That&#39;s a good thing. We can start at any random point in this space, and compute derivative, and slide along the direction where the E decreases. This algorithm is referred to as gradient descent. Do read more about it - it is simple and quite powerful for wide varieties of problems. . We will use pytorch to implement this regresison using gradient descent. Lets start. I learnt this while listening to a course in fast.ai. . import numpy as np import torch import matplotlib.pyplot as plt from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) import torch.nn as nn import torch.tensor as tensor . Create the dataset as a sum of a third degree polynomial and a little bit of uniform random noise. . # make the data n = 100 # number of samples of x x = torch.ones(n,4) # make a tensor x[:,0].uniform_(-1,1) # x x[:,1] = x[:,0]**2 x[:,2] = x[:,0]**3 a = tensor([1.,1,1,5]) # polynomial coefficients amax = 1 amin = -1 # create dataset y y = x@a + (amax-amin)*torch.rand(n)+amin # sum of polynomial and uniform random noise [-1,1] . So, Timmy has given us y and x. Let&#39;s visualize it on a scatter plot. We will also overlay on the scatter plot the samples from the underlying polynomial model governing this system. Ideally, we will like our polynomial regression to exactly match this polynomial model. If this happens then we have understood the system exactly. . fig = plt.subplots(figsize=(6,4)) ax = plt.subplot(1,1,1) ax.scatter(x[:,0],y,label=&#39;experiment samples&#39;) ax.scatter(x[:,0],x@a,label=&#39;underlying polynomial model samples&#39;) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.tick_params(which=&#39;both&#39;, width=2) ax.tick_params(which=&#39;major&#39;, length=7) # ax.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax.legend(loc=&#39;upper left&#39;,frameon=False,fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.set_xlabel(&#39;x&#39;,fontsize=14) ax.set_ylabel(&#39;y (x)&#39;,fontsize=14) plt.tight_layout() . Implementation . # Lets implement the error function def mse(y_hat,y): return((y_hat-y)**2).mean() . # Lets initialize our polynomial model a_hat = torch.tensor([1.,1.,0,0]) a_hat = nn.Parameter(a_hat) . # Lets define our gradient descent updates accumulate_loss = [] def update(): y_hat = x@a_hat # estimate y_hat loss = mse(y,y_hat) # compute error or loss accumulate_loss.append(loss) # if t % 10 == 0: print(loss) loss.backward() # compute derivative/grad with respect to each parameter with torch.no_grad(): # deactivate backprop a_hat.sub_(lr*a_hat.grad) # gradient descent step a_hat.grad.zero_() # reset grads lr = 1e-2 # this is learning rate parameter, if curious, do google more on it for t in range(1000): update() . Lets visualize the loss curve over updates, and the estimated model overlaid on the true model. . fig = plt.subplots(figsize=(14,4)) ax = plt.subplot(1,2,1) ax.plot(np.array(accumulate_loss)*1e6) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.tick_params(which=&#39;both&#39;, width=2) ax.tick_params(which=&#39;major&#39;, length=7) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.set_xlabel(&#39;update index&#39;,fontsize=14) ax.set_ylabel(&#39;Loss x 1e6&#39;,fontsize=14) plt.tight_layout() ax = plt.subplot(1,2,2) ax.scatter(x[:,0],y,label=&#39;experiment samples&#39;) ax.scatter(x[:,0],x@a,label=&#39;underlying polynomial model samples&#39;) ax.scatter(x[:,0],x@a_hat.detach().numpy(),color=&#39;g&#39;,label=&#39;estimated polynomial model samples&#39;) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.tick_params(which=&#39;both&#39;, width=2) ax.tick_params(which=&#39;major&#39;, length=7) # ax.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax.legend(loc=&#39;upper left&#39;,frameon=False,fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.set_xlabel(&#39;x&#39;,fontsize=14) ax.set_ylabel(&#39;y (x)&#39;,fontsize=14) plt.tight_layout() . From the above plots it is clear that we are headed in the descent direction (decresing loss or error). Further, our estimated polynomial model closely follows the underlying true polynomial model. . print(&#39;True model coefficients:&#39;) print(a.numpy()) print(&#39;Estimated model coefficients:&#39;) print(a_hat.detach().numpy()) . True model coefficients: [1. 1. 1. 5.] Estimated model coefficients: [1.1652924 0.9945537 0.5666001 5.0914917] . That&#39;s it . We saw that gradient descent can be used to estimate the coefficients of a linear regression model. Traditionally, polynomial regression is done using the Moore Penrose inverse. But we tried here using a NN approach and it too works and approximates the traditional solution. The flexibility here is we can use different differentiable error functions are obtain variants of the regression. .",
            "url": "https://neerajww.github.io/myblog/2020/06/15/polynomial_regress_GD.html",
            "relUrl": "/2020/06/15/polynomial_regress_GD.html",
            "date": " â€¢ Jun 15, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Implementing Fourier transform as a Neural Network",
            "content": "What is a Discrete Fourier transform (DFT) . I will start by assuming DFT is a black box. Our friend Tom has been kind enough to supply us with a dataset composed of inputs signals and corresponding output signals obtained from this black box. We know little bit about neural networks (NNs). So, lets use to model this black box. . Essentially, we will split the dataset given by Tommy into a train (80% samples) and a validation set (20% samples). Subsequently, using the training set we will train a NN to learn a mapping from input to output. The validation set will be used only for testing to verify if the mapping is generalizing to new data not seen by the NN during training. . (The idea presented here is something I asked Brandon Wu (the amazing!) to try on one day of his internship. Both of us jumped from our seats when we saw the matrix plot shown at the end). . Making the dataset . Let&#39;s get the data from Tom. . import numpy as np import matplotlib.pyplot as plt from numpy.fft import fft from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) dim_signal = 256 dataset_samples = 5000 # 256x1 dimension white noise vectors data = [np.random.randn(1, dim_signal).T for _ in range(dataset_samples)] # real(DFT(data)) cosData = [np.real(fft(x,axis=0)) for x in data] cosData = np.array(cosData) cosData = cosData.reshape(cosData.shape[0], cosData.shape[1]) # imaginary(DFT(data)) sinData = [np.imag(fft(x,axis=0)) for x in data] sinData = np.array(sinData) sinData = sinData.reshape(sinData.shape[0], sinData.shape[1]) . Let&#39;s see an example from our dataset. . fig = plt.subplots(figsize=(16,3)) ax = plt.subplot(1,3,1) plt.plot(data[0],color=&#39;blue&#39;,label=&#39;SIGNAL&#39;) ax.set_xlabel(&#39;SAMPLE INDEX&#39;) ax.set_ylabel(&#39;A.U.&#39;) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.tick_params(which=&#39;both&#39;, width=2) ax.tick_params(which=&#39;major&#39;, length=7) ax.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax.legend(loc=&#39;lower right&#39;,frameon=False,fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.figure.savefig(&#39;signal.pdf&#39;, bbox_inches=&#39;tight&#39;) ax = plt.subplot(1,3,2) plt.plot(cosData[0],color=&#39;red&#39;,label=&#39;REAL[DFT]&#39;) ax.set_xlabel(&#39;SAMPLE INDEX&#39;) ax.set_ylabel(&#39;A.U.&#39;) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.tick_params(which=&#39;both&#39;, width=2) ax.tick_params(which=&#39;major&#39;, length=7) ax.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax.legend(loc=&#39;lower right&#39;,frameon=False,fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.figure.savefig(&#39;signal_DFT_real.pdf&#39;, bbox_inches=&#39;tight&#39;) ax = plt.subplot(1,3,3) plt.plot(sinData[0],color=&#39;green&#39;,label=&#39;IMAG[DFT]&#39;) ax.set_xlabel(&#39;SAMPLE INDEX&#39;) ax.set_ylabel(&#39;A.U.&#39;) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.tick_params(which=&#39;both&#39;, width=2) ax.tick_params(which=&#39;major&#39;, length=7) ax.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax.legend(loc=&#39;lower right&#39;,frameon=False,fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.figure.savefig(&#39;signal_DFT_imag.pdf&#39;, bbox_inches=&#39;tight&#39;) plt.show() . Our Neural Network . The output of a DFT is a complex signal. We will model this as a sum of a real and an imaginary part. This way our NN will be dealing only with real values. . . Step 1: build the NN structure . # import pytorch packages import torch from torch.utils.data import Dataset, DataLoader from torch import nn from torch.autograd import Variable import time class fourier(nn.Module): def __init__(self): super(fourier, self).__init__() self.sinLayer = nn.Linear(256, 256) self.cosLayer = nn.Linear(256, 256) def forward(self, x): y1 = self.sinLayer(x) y2 = self.cosLayer(x) return y1, y2 class FourierDataset(Dataset): def __init__(self, data, output1, output2): indx = int(0.8 * len(data)) first = [(data[i], output1[i], output2[i]) for i in range(len(data)) if i &lt; indx] second = [(data[i], output1[i], output2[i]) for i in range(len(data)) if i &gt;= indx] self.data = first self.validation = second def __len__(self): return len(self.data) def __getitem__(self, idx): return self.data[idx] def nLoss(lossFun, one, two): print(1) loss = 0 for i in range(len(one)): print(2) loss += lossFun(one[i], two[i]).numpy() print(3) return loss . Step 2: Initialize the NN training parameters . num_epochs = 800 batch_size = 128 learning_rate = 1e-4 Fourier = fourier() dataset = FourierDataset(data, sinData, cosData) dataloader = DataLoader(dataset.data, batch_size=batch_size, shuffle=True) . optimizer = torch.optim.Adam( Fourier.parameters(), lr=learning_rate, weight_decay=1e-5) criterion = nn.MSELoss() losses = [] vlosses = [] regs = [] # make the validation data tensor validData = [] validSin = [] validCos = [] for dataIns in dataset.validation: validData.append(dataIns[0]) validSin.append(dataIns[1]) validCos.append(dataIns[2]) validSin = torch.from_numpy(np.array(validSin)) validCos = torch.from_numpy(np.array(validCos)) validData = torch.from_numpy(np.array(validData)) validData = validData.reshape(validData.shape[0],1,256) . Step 3: Train the NN using forward pass of training data and backward pass of trianing error updating the NN weights. This process is iterated num_epochs times. . # we will monitor a sample (indx) from the validation set examp_cos_output = [] examp_sin_output = [] indx = 10 weightSinLayer = [] # train the NN for num_epochs start_time = time.time() for epoch in range(num_epochs): for dataIns in dataloader: img, sinCorrect, cosCorrect = dataIns sinCorrect = sinCorrect.reshape(sinCorrect.shape[0], sinCorrect.shape[1]) cosCorrect = cosCorrect.reshape(cosCorrect.shape[0], cosCorrect.shape[1]) img = img.view(img.size(0), -1) img = img.reshape(img.shape[0],1,256) # ===================forward===================== sinOutput, cosOutput = Fourier(img.float()) loss1 = criterion(sinOutput.reshape(sinOutput.shape[0], sinOutput.shape[2]), sinCorrect.float()) loss2 = criterion(cosOutput.reshape(cosOutput.shape[0], cosOutput.shape[2]), cosCorrect.float()) loss = loss1 + loss2 # ===================backward==================== optimizer.zero_grad() loss.backward() optimizer.step() # ===================forward===================== sinOutput2, cosOutput2 = Fourier(validData.float()) loss1 = criterion(sinOutput2.reshape(sinOutput2.shape[0], sinOutput2.shape[2]), validSin.float()) loss2 = criterion(cosOutput2.reshape(cosOutput2.shape[0], cosOutput2.shape[2]), validCos.float()) vloss = loss1 + loss2 # ===================store loss================== losses.append(loss.data.numpy()) vlosses.append(vloss.data.numpy()) if epoch % 50 == 0: print(&#39;epoch [{}/{}], loss:{:.4f}, validation_loss:{:.4f}, time_elapsed:{:.4f}&#39; .format(epoch + 1, num_epochs, loss.data, vlosses[-1], time.time() - start_time)) if vlosses[-1] &lt;= 0.0001: print(vlosses[-1]) break # ===================store and example signal========== examp_cos_output.append(cosOutput2.detach().numpy()[indx]) examp_sin_output.append(sinOutput2.detach().numpy()[indx]) # =================== store the sine layer 256x256 weight matix ========== for i, param in enumerate(Fourier.parameters()): if i == 0: a = param.detach().numpy().copy() weightSinLayer.append(a) . epoch [1/800], loss:258.2382, validation_loss:255.5916, time_elapsed:0.1411 epoch [51/800], loss:196.6595, validation_loss:200.4876, time_elapsed:7.2933 epoch [101/800], loss:147.5006, validation_loss:154.0908, time_elapsed:14.3655 epoch [151/800], loss:110.7806, validation_loss:114.9219, time_elapsed:20.8714 epoch [201/800], loss:76.2455, validation_loss:82.2963, time_elapsed:27.5049 epoch [251/800], loss:51.3001, validation_loss:55.8251, time_elapsed:36.9585 epoch [301/800], loss:32.0512, validation_loss:35.1757, time_elapsed:46.5460 epoch [351/800], loss:17.3773, validation_loss:19.9666, time_elapsed:56.1011 epoch [401/800], loss:8.4049, validation_loss:9.6959, time_elapsed:65.5383 epoch [451/800], loss:2.8125, validation_loss:3.6714, time_elapsed:75.0106 epoch [501/800], loss:0.6671, validation_loss:0.9133, time_elapsed:84.1194 epoch [551/800], loss:0.0897, validation_loss:0.1225, time_elapsed:91.5053 epoch [601/800], loss:0.0154, validation_loss:0.0191, time_elapsed:97.9016 epoch [651/800], loss:0.0030, validation_loss:0.0049, time_elapsed:104.8059 epoch [701/800], loss:0.0009, validation_loss:0.0010, time_elapsed:112.0194 epoch [751/800], loss:0.0005, validation_loss:0.0005, time_elapsed:118.6301 . Let&#39;s visulaize the train and validation losses as a function of epoch counts. We will like this to decrease monotonically, and it indeed does so. This implies the NN is learning the mapping, and also generalizing it to the validations set. . FS = 14 fig, ax = plt.subplots(figsize=(7,4)) ax.plot(losses, label = &#39;TRANING LOSS&#39;) ax.plot(vlosses,color=&#39;orange&#39;,label=&#39;VALIDATION LOSS&#39;) ax.set_ylabel(&#39;MSE&#39;,fontsize=FS) ax.set_xlabel(&#39;EPOCH&#39;,fontsize=FS) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.tick_params(which=&#39;both&#39;, width=2) ax.tick_params(which=&#39;major&#39;, length=7) ax.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.show() . Step 4: Lets also see an example signal to visualize the learnt fit on a signal sample from validation data. . fig = plt.subplots(figsize=(12, 3)) indx = 10 ax = plt.subplot(1,2,1) thisOutput = sinOutput.detach().numpy() thisCorrect = sinCorrect.detach().numpy() ax.plot(thisOutput[indx].T, label=&#39;output of network&#39;) ax.plot(thisCorrect[indx].T, color=&#39;orange&#39;, label=&#39;actual fft of data&#39;) ax.legend() ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.tick_params(which=&#39;both&#39;, width=2) ax.tick_params(which=&#39;major&#39;, length=7) ax.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax.legend(loc=&#39;lower right&#39;,frameon=False,fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax = plt.subplot(1,2,2) thisOutput = cosOutput.detach().numpy() thisCorrect = cosCorrect.detach().numpy() ax.plot(thisOutput[indx].T, label=&#39;output of network&#39;) ax.plot(thisCorrect[indx].T, color=&#39;orange&#39;, label=&#39;actual fft of data&#39;) ax.legend() ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.tick_params(which=&#39;both&#39;, width=2) ax.tick_params(which=&#39;major&#39;, length=7) ax.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax.legend(loc=&#39;lower right&#39;,frameon=False,fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.show() . Lets also visualize how the NN approximates the mapping over epochs. We will take a signal from the validation set and plot it estimated DFT using NN over the epochs.Below is the gif. . . Below is the code snippet I used to generate the video to make the above gif. . Lets now understand the weight learnt by the NN. As this NN has no nonlinearity hence, visualizing the weights can give us a good understanding of what the NN is doing here. . fig, axes = plt.subplots(1, 2, figsize=(12, 7)) for i, param in enumerate(Fourier.parameters()): if i == 0: axes[0].imshow(param.detach().numpy(),cmap=&#39;RdBu_r&#39;,vmin=-1, vmax=1) axes[0].set_title(&#39;sine layer weight matrix&#39;) elif i == 2: pos = axes[1].imshow(param.detach().numpy(),cmap=&#39;RdBu_r&#39;,vmin=-1, vmax=1) axes[1].set_title(&#39;cosine layer weight matrix&#39;) fig.tight_layout() fig.subplots_adjust(right=0.8) cbar_ax = fig.add_axes([0.85, 0.25, 0.015, 0.5]) #(xloc,yloc,width,height) fig.colorbar(pos, cax=cbar_ax,ticks=[-1,-0.5,0,0.5,1]) plt.show() . A epoch-wise evolution of the sine layer weight matrix is shown below. . Lets see few columns of the sine and cosine matrix . fig, axes = plt.subplots(1, 2, figsize=(12, 4)) for i, param in enumerate(Fourier.parameters()): if i == 0: axes[0].plot(param.detach().numpy()[0],color=&#39;b&#39;) axes[0].plot(param.detach().numpy()[1],color=&#39;r&#39;) axes[0].plot(param.detach().numpy()[2],color=&#39;g&#39;) axes[0].set_title(&#39;sine matrix columns&#39;) axes[0].spines[&#39;right&#39;].set_visible(False) axes[0].spines[&#39;top&#39;].set_visible(False) elif i == 2: axes[1].plot(param.detach().numpy()[0],color=&#39;b&#39;) axes[1].plot(param.detach().numpy()[1],color=&#39;r&#39;) axes[1].plot(param.detach().numpy()[2],color=&#39;g&#39;) axes[1].set_title(&#39;cosine matrix columns&#39;) axes[1].spines[&#39;right&#39;].set_visible(False) axes[1].spines[&#39;top&#39;].set_visible(False) plt.show() . That&#39;s it . Each row in the above matrices are approximating the basis vectors corresponding to sine and cosine waves in traditional DFT matrix. Thus, the NN, learning a linear transformation here, converges to the unique solution, that is, the DFT matrix here. So we learnt that, . the DFT black box is composed of these matrices | the matrices are composed of sine and cosine signals | the MSE between traditional DFT and NN implementation decreases over epochs | . Appendices . # code for spectrum fit animation trueSpectrum = 10*np.log10(validCos.detach().numpy()[indx]**2+validSin.detach().numpy()[indx]**2) from matplotlib import animation, rc rc(&#39;animation&#39;,html=&#39;jshtml&#39;) fig = plt.figure(figsize=(7,4)) ax = plt.subplot(1,1,1) plt.plot(np.arange(0,256,1),trueSpectrum,color=&#39;b&#39;,label=&#39;TRUE&#39;) plt.xlabel(&#39;DFT BIN&#39;,fontsize=14) plt.ylabel(&#39;PSD [in dB]&#39;,fontsize=14) line, = plt.plot(np.arange(0,256,1),np.zeros((256,)),color=&#39;r&#39;,label=&#39;ESTIMATED&#39;) plt.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.legend(loc=&#39;lower right&#39;,frameon=False,fontsize=14) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.close() def animate(i): outputSpectrum = 10*np.log10(examp_cos_output[i][0]**2+examp_sin_output[i][0]**2) line.set_ydata(outputSpectrum) return line, anim = animation.FuncAnimation(fig,animate,np.arange(0,400), interval=20) Writer = animation.writers[&#39;ffmpeg&#39;] writer = Writer(fps=10, metadata=dict(artist=&#39;Me&#39;), bitrate=1800) anim.save(&#39;anim_1.mp4&#39;, writer=writer) # code for matrix video import matplotlib.animation as animation # First set up the figure, the axis, and the plot element we want to animate fig = plt.figure( figsize=(5,5) ) im = plt.imshow(np.zeros((256,256)), aspect=&#39;auto&#39;, vmin=-1, vmax=1,cmap=&#39;RdBu_r&#39;) plt.title(&#39;sine layer wieght matrix over epochs&#39;) def animate_func(i): im.set_array(weightSinLayer[i]) return [im] anim = animation.FuncAnimation( fig, animate_func, frames = np.arange(0,799), interval = 20, # in ms ) anim.save(&#39;anim_2.mp4&#39;, fps=fps, extra_args=[&#39;-vcodec&#39;, &#39;libx264&#39;]) print(&#39;Done!&#39;) .",
            "url": "https://neerajww.github.io/myblog/2020/06/14/Fourier_network.html",
            "relUrl": "/2020/06/14/Fourier_network.html",
            "date": " â€¢ Jun 14, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Tips for making plots",
            "content": "About . This python notebook is a demonstration of some simple codes to make neat plots. I have used these to make plots for my research papers. Some of my friends liked them so I thought to share some tips in this post. I will keep it short and to the point. Also, there are lots of amazing tutorials in the web to make wonderful plots with python. So, don&#39;t just stop here if you dont find what you are looking for. . A Line Plot . Let&#39;s start by plotting some data. . import numpy as np import matplotlib.pyplot as plt . mean, std = 0, 1 num_samples = 1000 y = np.random.normal(mean, std, size=num_samples) plt.plot(y) plt.show() . On staring the above plot for a minute you will easily spot several things which can be improved. The key is to know the terminology associated with the anatomy of a matplotlib plot. Once you know the terms, a simple searching on internet will help you how to incorporate anything you wish into this plot. So, here is the anotomy. . Let&#39;s improve the plot now. . # we import one more package to make minor ticks from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator) fig = plt.subplots(figsize=(16,5)) # (width_in_cms, height_in_cms) # plotting without any care ax = plt.subplot(1,2,1) ax.plot(y) # plotting wiith some care ax = plt.subplot(1,2,2) ax.plot(y) ax.set_xlabel(&#39;SAMPLE INDEX&#39;,fontsize=14) ax.set_ylabel(&#39;A.U.&#39;,fontsize=14) # A.U stands for Arbitrary Units ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.tick_params(which=&#39;both&#39;, width=2) ax.tick_params(which=&#39;major&#39;, length=7) ax.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.show() . A twin axis line plot . Let&#39;s see by plotting some data. We will also add the p-scrore comparing two bars. . # Create sin and cosine fs = 1000 t = np.arange(0,8000,1)/fs y1 = np.sin(t) y2 = np.cos(t) fig, ax1 = plt.subplots(figsize=(9,4)) color = &#39;tab:red&#39; ax1.set_xlabel(&#39;TIME [in secs]&#39;) ax1.set_ylabel(&#39;sin(t)&#39;, color=color, fontsize=14) ax1.plot(t,y1, color=color,alpha=0.7) # alpha controls the opacity ax1.tick_params(axis=&#39;y&#39;, labelcolor=color) ax1.spines[&#39;top&#39;].set_visible(False) ax1.grid(True) ax1.xaxis.set_minor_locator(AutoMinorLocator()) ax1.yaxis.set_minor_locator(AutoMinorLocator()) ax1.tick_params(which=&#39;both&#39;, width=2) ax1.tick_params(which=&#39;major&#39;, length=7) ax1.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) plt.xticks(fontsize=13) plt.yticks(fontsize=13) # plt.xticks([0,31,60,91,len(sorteddates)-1], # [&#39;11 Jan&#39;,&#39;11 Feb&#39;,&#39;11 Mar&#39;,&#39;11 Apr&#39;,&#39;16 May 2020&#39;],rotation=0) ax2 = ax1.twinx() # instantiate a second axes that shares the same x-axis color = &#39;tab:blue&#39; ax2.set_ylabel(&#39;cos(t)&#39;, color=color,fontsize=14) # we already handled the x-label with ax1 ax2.plot(t,y2,color=color,alpha=0.5) ax2.tick_params(axis=&#39;y&#39;, labelcolor=color) ax2.spines[&#39;top&#39;].set_visible(False) ax1.grid(True) ax2.xaxis.set_minor_locator(AutoMinorLocator()) ax2.yaxis.set_minor_locator(AutoMinorLocator()) ax2.tick_params(which=&#39;both&#39;, width=2) ax2.tick_params(which=&#39;major&#39;, length=7) ax2.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) plt.xticks(fontsize=13) plt.yticks(fontsize=13) # plt.xticks([0,31,60,91,len(sorteddates)-1], # [&#39;11 Jan&#39;,&#39;11 Feb&#39;,&#39;11 Mar&#39;,&#39;11 Apr&#39;,&#39;16 May 2020&#39;],rotation=0) fig.tight_layout() # otherwise the right y-label is slightly clipped plt.show() . A bar plot . Bar plots are useful when we have few variables to plot on x-axis and corresponding values in y-axis. Let&#39;s plot some. First we will define a function to annotate the p-value on top of the bars. . # First we will define a function to show significance values. # I pulled this from internet some time back and now can&#39;t find the reference. If you find do find it, let me know, I will like to add an acknowledgement. # funcs definitions to make significant plot markers def barplot_annotate_brackets(num1, num2, data, center, height, yerr=None, dh=.05, barh=.05, hdist=1,fs=None, maxasterix=None,fsize=14): &quot;&quot;&quot; Annotate barplot with p-values. :param num1: number of left bar to put bracket over :param num2: number of right bar to put bracket over :param data: string to write or number for generating asterixes :param center: centers of all bars (like plt.bar() input) :param height: heights of all bars (like plt.bar() input) :param yerr: yerrs of all bars (like plt.bar() input) :param dh: height offset over bar / bar + yerr in axes coordinates (0 to 1) :param barh: bar height in axes coordinates (0 to 1) :param fs: font size :param maxasterix: maximum number of asterixes to write (for very small p-values) &quot;&quot;&quot; if type(data) is str: text = data else: # * is p &lt; 0.05 # ** is p &lt; 0.005 # *** is p &lt; 0.0005 # etc. text = &#39;&#39; p = .05 while data &lt; p: text += &#39;*&#39; p /= 10. if maxasterix and len(text) == maxasterix: break if len(text) == 0: text = &#39;n. s.&#39; lx, ly = center[num1], height[num1] rx, ry = center[num2], height[num2] if yerr: ly += yerr[num1] ry += yerr[num2] ax_y0, ax_y1 = plt.gca().get_ylim() dh *= (ax_y1 - ax_y0) barh *= (ax_y1 - ax_y0) y = max(ly, ry) + dh barx = [lx, lx, rx, rx] bary = [y, y+barh, y+barh, y] mid = ((lx+rx)/2, y+barh+hdist) plt.plot(barx, bary, c=&#39;black&#39;) kwargs = dict(ha=&#39;center&#39;, va=&#39;bottom&#39;) if fs is not None: kwargs[&#39;fontsize&#39;] = fs plt.text(*mid, text, **kwargs,fontsize=fsize) . Now we will make the bar plot. . # make data x = [] x.append(np.random.normal(10, std, size=num_samples)) x.append(5+x[0]) # scatter plots fig = plt.subplots(figsize=(9, 4)) ax = plt.subplot(1,2,1) ax.scatter(x[0],x[1],color=&#39;green&#39;) ax.set_xlabel(&#39;VAR 1&#39;,fontsize=14) ax.set_ylabel(&#39;VAR 2&#39;,fontsize=14) ax.xaxis.set_minor_locator(AutoMinorLocator()) ax.yaxis.set_minor_locator(AutoMinorLocator()) ax.tick_params(which=&#39;both&#39;, width=2) ax.set_xlim(5,20) ax.set_ylim(5,20) ax.grid(True) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # ax.plot([5,60],[5,60],&#39;--&#39;,color=&#39;black&#39;,alpha=0.25) ax.tick_params(which=&#39;minor&#39;, length=4, color=&#39;gray&#39;) ax = plt.subplot(1,2,2) ax.bar(2,np.mean(x[0]),yerr=np.std(x[0]), align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot; &quot;,color=&#39;red&#39;,label=&#39;VAR 1&#39;,width=.5) ax.bar(4,np.mean(x[1]),yerr=np.std(x[1]), align=&#39;center&#39;,alpha=1, ecolor=&#39;black&#39;,capsize=5,hatch=&quot;//&quot;,color=&#39;blue&#39;,label=&#39;VAR 2&#39;,width=.5) ax.set_ylabel(&#39;AVERAGE&#39;,fontsize=14) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=14) plt.xticks([2,4], [&#39;VAR 1&#39;,&#39;VAR 2&#39;],rotation=0) ax.set_xlim(1,7) ax.set_ylim(5,19) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # sns.despine() # Call the function barplot_annotate_brackets(0, 1, &#39;p = dummy&#39;, [2,4],[np.mean(x[0]),np.mean(x[1])], dh=.1,barh=.05,fsize=14) plt.tight_layout() plt.show() . A density plot . # here we will use the seaborn package import seaborn as sns sns.set() # Use seaborn&#39;s default style to make attractive graphs sns.set_style(&quot;white&quot;) sns.set_style(&quot;ticks&quot;) fig = plt.subplots(figsize=(8,3)) ax = plt.subplot(1,1,1) sns.distplot(x[0],label=&#39;VAR 1&#39;,color=&#39;red&#39;) sns.distplot(x[1],label=&#39;VAR 2&#39;,color=&#39;blue&#39;) # sns.kdeplot(np.reciprocal(rt_spkr_2[0]), shade=True,color=&#39;red&#39;,label=&#39;eng&#39;) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.set_xlabel(&#39;A.U&#39;,fontsize=13) ax.set_ylabel(&#39;DENSITY&#39;,fontsize=13) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=13) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.show() . A 2-D image or spectrogram plot . We will first read a sound file (WAV format). Then we will compute its spectrogram, and follow this up with plotting the time-domain signal and the spectrogram. . from scipy.io import wavfile # package to read WAV file from mpl_toolkits.axes_grid1 import make_axes_locatable # to move placement of colorbar # function to create spectrogram def generate_spectrogram(x,fs,wdur=20e-3,hdur=5e-3): X = [] i = 0 cnt = 0 win = np.hamming(wdur*fs) win = win - np.min(win) win = win/np.max(win) while i&lt;(len(x)-int(wdur*fs)): X.append(np.multiply(win,x[i:(i+int(wdur*fs))])) i = i + int(hdur*fs) cnt= cnt+1 X = np.array(X) Xs = abs(np.fft.rfft(X)) return Xs # read WAV file and plot data [fs, x] = wavfile.read(&#39;./my_sounds/count.wav&#39;) sig = x/np.max(np.abs(x)) taxis = np.arange(0,len(x))/fs fig = plt.subplots(figsize=(6,1)) ax = plt.subplot(1,1,1) ax.plot(taxis,sig) ax.set_xlim(taxis[0]-0.1/2,taxis[-1]) ax.set_ylim(-1,1) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U&#39;) sns.despine(offset = .1,trim=False) # fmt=&#39;png&#39; # plt.savefig(path_store_figure+&#39;IIScConnect_sample_count_sig.&#39;+fmt, dpi=None, facecolor=&#39;w&#39;, edgecolor=&#39;w&#39;, # orientation=&#39;portrait&#39;, papertype=None, format=fmt,transparent=False, bbox_inches=&#39;tight&#39;, pad_inches=None, metadata=None) plt.show() fig, ax = plt.subplots(figsize=(6,4)) Xs = generate_spectrogram(sig,fs,wdur=25e-3,hdur=2.5e-3) XdB = 20*np.log10(Xs.T) XdB = XdB - np.max(XdB) im = ax.imshow(XdB,origin=&#39;lower&#39;,aspect=&#39;auto&#39;,extent = [taxis[0], taxis[-1], 0, fs/2/1e3], cmap=&#39;RdBu_r&#39;,vmin = 0, vmax =-100) divider = make_axes_locatable(ax) colorbar_ax = fig.add_axes([.95, 0.1, 0.015, 0.5]) fig.colorbar(im, cax=colorbar_ax) ax.set_xlim(taxis[0]-0.1/2,taxis[-1]) ax.set_ylim(-.1,4) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;FREQ [in kHz]&#39;) sns.despine(offset = 0.01,trim=False) # plt.savefig(path_store_figure+&#39;IIScConnect_sample_count_spectgm.&#39;+fmt, dpi=None, facecolor=&#39;w&#39;, edgecolor=&#39;w&#39;, # orientation=&#39;portrait&#39;, papertype=None, format=fmt,transparent=False, bbox_inches=&#39;tight&#39;, pad_inches=None, metadata=None) plt.show() . /Users/neeks/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:22: WavFileWarning: Chunk (non-data) not understood, skipping it. . A confusion matrix . cf_matrix = np.random.normal(0,1,(5,5)) keys = [&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;] fig = plt.subplots(figsize=(7,5)) ax = plt.subplot(1,1,1) # sns.set(font_scale=1.4)#for label size sns.heatmap(cf_matrix/np.sum(cf_matrix)*100, annot=True, fmt=&#39;.2g&#39;, cmap=&#39;Blues&#39;, annot_kws={&quot;size&quot;: 13}, cbar_kws={&#39;label&#39;: &#39;RANDOM NUMBERS&#39;})# font size ax.figure.axes[-1].yaxis.label.set_size(10) # fontsize for label on color bar ax.set_xticks(np.arange(len(keys))) ax.set_yticks(np.arange(len(keys))) ax.set_xticklabels(keys,rotation=0,fontsize=13) ax.set_yticklabels(keys,rotation=0,fontsize=13) plt.show() . Adding plot into a paper . The key here is to first create the plot at an aspect ratio as you will like it in the paper. I do this by setting the figsize to appropriate dimensions. . fig = plt.subplots(figsize=(6,4)) # (width_cms,height_cms) . You can also resize the figure in latex but that doesn&#39;t look very nice as the text and numbers inside the figure don&#39;t get appropriately scaled. From python, I save figure as PDF using: . ax.figure.savefig(&#39;name.pdf&#39;, bbox_inches=&#39;tight&#39;) . For more options, there is this: . fmt=&#39;pdf&#39; plt.savefig(&#39;name.&#39;+fmt, dpi=None, facecolor=&#39;w&#39;, edgecolor=&#39;w&#39;, orientation=&#39;portrait&#39;, papertype=None, format=fmt,transparent=False, bbox_inches=&#39;tight&#39;, pad_inches=None, metadata=None) . Sometimes I have to create multiple subplots and also block diagrams. For this I open Keynote (in Mac), and insert the plots (and make any block diagrams) in a slide. Then I export the slide as a PDF (saving in Best form). Subsequently, I crop the white spaces around the exported PDF using pdfcrop command in terminal. Done. . Adding plot into a slide or webpage . I guess JPEG is the smallest file size for a plot/figure. The downside is JPEG is not vector scalable graphics. When you zoom into a JPEG image you will loose on the resolution and see block artifacts, This is not there in PDF and EPS formats. Hence, PDF and EPS format suit academic papers and JPEG/PNG dont. However, JPEG and PNG are good for slides and webpages as you dont want a huge filesize here. . That&#39;s it! . What I presented is some simple codes to make neat plots. These plots are basic line/bar/distribution plots. The matplotlib is quite resourceful to make many more elegant plots. So, if you imagine something, the next step will be to know the term for it, and then see the documentation of matplotlib (or google it) and you may find a lead. .",
            "url": "https://neerajww.github.io/myblog/2020/06/11/plotting_tips.html",
            "relUrl": "/2020/06/11/plotting_tips.html",
            "date": " â€¢ Jun 11, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Using Hilbert transform to get signal envelope",
            "content": "First we import some packages. . import numpy as np import scipy.signal as signal import matplotlib.pyplot as plt import seaborn as sns; sns.set() sns.set_style(&quot;white&quot;) sns.set_style(&quot;ticks&quot;) . Next, we define functions to create envelope and carrier signals. . def create_envelope(num_samples,fL_hz=100,N=1000,beta=3): # we will create the envelope by low pass filtering white noise mean = 0 std = 1 taps = signal.firwin(N, fL_hz/nyq_rate, window=(&#39;kaiser&#39;, beta)) x = np.random.normal(mean, std, size=num_samples) x = signal.filtfilt(taps, 1,x) x = (x-np.min(x)) x = x/np.max(x) return x def create_carrier_harmonic(fs=8e3,fc=200,num_samples=1000,ncomps=1): # this will create a harmonic carrier with ncomps harmonics x = [] for i in range(ncomps): x.append(np.sin(2*np.pi*(i+1)*fc*np.arange(0,num_samples,1)/fs)) x = sum(x) x = x/np.max(np.abs(x)) return x def create_carrier_noise(mean=0,std=1,num_samples=1000): # this will create a white noise carrier x = np.random.normal(mean, std, size=num_samples) x = x/np.abs(x) return x . Amplitude modulated tone . Let&#39;s start with an amplitude modulated tone and estimate the envelope. . # init filter and signal params fs = 8e3 dur = 5 # signal duration num_samples = int(dur*fs) t = np.arange(0,num_samples,1)/fs nyq_rate = fs / 2.0 width = 5.0/nyq_rate # 5 Hz filter transition width. ripple_db = 60.0 # stop band attenuation N, beta = signal.kaiserord(ripple_db, width) # create envelope fL_hz = 50 envelope = create_envelope(num_samples=num_samples,fL_hz=fL_hz,N=N,beta=beta) # create carrier carrier = create_carrier_harmonic(fs=fs,fc=100,num_samples=num_samples,ncomps=1) # create am-fm signal x = np.multiply(envelope,carrier) # estimate analytic signal ax = signal.hilbert(x) envelope_hat = np.abs(ax) # plt.plot(x) fig = plt.subplots(figsize=(8,4)) ax = plt.subplot(1,1,1) ax.plot(t,x,color=&#39;black&#39;,label=&#39;SIGNAL&#39;) ax.plot(t,envelope,color=&#39;blue&#39;,label=&#39;ENVP. TRUE&#39;) ax.plot(t,envelope_hat,color=&#39;red&#39;,label=&#39;ENVP. EST.&#39;) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U.&#39;) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=12) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.xlim(0,0.5) plt.ylim(-1,1.75) plt.show() . Amplitude modulated harmonics . Let&#39;s make the carrier a sum of 2 harmonics. The carrier is not a tone but a sum of two tones. . # init filter and signal params fs = 8e3 dur = 5 # signal duration num_samples = int(dur*fs) t = np.arange(0,num_samples,1)/fs nyq_rate = fs / 2.0 width = 5.0/nyq_rate # 5 Hz filter transition width. ripple_db = 60.0 # stop band attenuation N, beta = signal.kaiserord(ripple_db, width) # create envelope fL_hz = 50 envelope = create_envelope(num_samples=num_samples,fL_hz=fL_hz,N=N,beta=beta) # create carrier carrier = create_carrier_harmonic(fs=fs,fc=100,num_samples=num_samples,ncomps=2) # create am-fm signal x = np.multiply(envelope,carrier) # estimate analytic signal ax = signal.hilbert(x) envelope_hat = np.abs(ax) # plt.plot(x) fig = plt.subplots(figsize=(8,4)) ax = plt.subplot(1,1,1) ax.plot(t,x,color=&#39;black&#39;,label=&#39;SIGNAL&#39;) ax.plot(t,envelope,color=&#39;blue&#39;,label=&#39;ENVP. TRUE&#39;) ax.plot(t,envelope_hat,color=&#39;red&#39;,label=&#39;ENVP. EST.&#39;) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U.&#39;) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=12) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.xlim(0,0.5) plt.ylim(-.5,1) plt.show() . The envelope estimated is a poor fit to the true envelope. Can we improve this? One option is to apply a lowpass filter to the estimated envelope. Lets see how does it do. . # init filter and signal params fs = 8e3 dur = 5 # signal duration num_samples = int(dur*fs) t = np.arange(0,num_samples,1)/fs nyq_rate = fs / 2.0 width = 5.0/nyq_rate # 5 Hz filter transition width. ripple_db = 60.0 # stop band attenuation N, beta = signal.kaiserord(ripple_db, width) # create envelope fL_hz = 50 envelope = create_envelope(num_samples=num_samples,fL_hz=fL_hz,N=N,beta=beta) # create carrier carrier = create_carrier_harmonic(fs=fs,fc=100,num_samples=num_samples,ncomps=2) # create am-fm signal x = np.multiply(envelope,carrier) # estimate analytic signal ax = signal.hilbert(x) envelope_hat = np.abs(ax) taps = signal.firwin(N, fL_hz/nyq_rate, window=(&#39;kaiser&#39;, beta)) envelope_hat_filt = signal.filtfilt(taps, 1,envelope_hat) fig = plt.subplots(figsize=(16,4)) ax = plt.subplot(1,2,1) ax.plot(t,x,color=&#39;black&#39;,label=&#39;SIGNAL&#39;) ax.plot(t,envelope,color=&#39;blue&#39;,label=&#39;ENVP. TRUE&#39;) ax.plot(t,envelope_hat,color=&#39;red&#39;,label=&#39;ENVP. EST.&#39;) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U.&#39;) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=12) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.xlim(0,0.5) plt.ylim(-.5,1) ax = plt.subplot(1,2,2) ax.plot(t,x,color=&#39;black&#39;,label=&#39;SIGNAL&#39;) ax.plot(t,envelope,color=&#39;blue&#39;,label=&#39;ENVP. TRUE&#39;) ax.plot(t,envelope_hat_filt,color=&#39;red&#39;,label=&#39;ENVP. EST. FILT.&#39;) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U.&#39;) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=12) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.xlim(0,0.5) plt.ylim(-.5,1) plt.show() . Amplitude modulated white noise . We do better with filter but still not perfect. Now, lets make the carrier a broadband white noise instead of tones. . # init filter and signal params fs = 8e3 dur = 5 # signal duration num_samples = int(dur*fs) t = np.arange(0,num_samples,1)/fs nyq_rate = fs / 2.0 width = 5.0/nyq_rate # 5 Hz filter transition width. ripple_db = 60.0 # stop band attenuation N, beta = signal.kaiserord(ripple_db, width) # create envelope fL_hz = 50 envelope = create_envelope(num_samples=num_samples,fL_hz=fL_hz,N=N,beta=beta) # create carrier carrier = create_carrier_noise(mean=0,std=1,num_samples=num_samples) # create am-fm signal x = np.multiply(envelope,carrier) # estimate analytic signal ax = signal.hilbert(x) envelope_hat = np.abs(ax) taps = signal.firwin(N, fL_hz/nyq_rate, window=(&#39;kaiser&#39;, beta)) envelope_hat_filt = signal.filtfilt(taps, 1,envelope_hat) fig = plt.subplots(figsize=(16,4)) ax = plt.subplot(1,2,1) ax.plot(t,x,color=&#39;black&#39;,label=&#39;SIGNAL&#39;) ax.plot(t,envelope,color=&#39;blue&#39;,label=&#39;ENVP. TRUE&#39;) ax.plot(t,envelope_hat,color=&#39;red&#39;,label=&#39;ENVP. EST.&#39;) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U.&#39;) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=12) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.xlim(0,0.5) plt.ylim(-2,2) ax = plt.subplot(1,2,2) ax.plot(t,x,color=&#39;black&#39;,label=&#39;SIGNAL&#39;) ax.plot(t,envelope,color=&#39;blue&#39;,label=&#39;ENVP. TRUE&#39;) ax.plot(t,envelope_hat_filt,color=&#39;red&#39;,label=&#39;ENVP. EST. FILT.&#39;) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U.&#39;) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=12) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) plt.xlim(0,0.5) plt.ylim(-2,2) plt.show() . Again, we see that filtered envelope is a better estimate. Now let&#39;s use this approach to obtain an envelope of speech signal. . Speech signal . from scipy.io import wavfile # package to read WAV file # read WAV file and plot data [fs, x] = wavfile.read(&#39;./my_sounds/count.wav&#39;) x = x/np.max(np.abs(x)) t = np.arange(0,len(x))/fs # get analytic signal ax = signal.hilbert(x) envelope_hat = np.abs(ax) taps = signal.firwin(N, fL_hz/nyq_rate, window=(&#39;kaiser&#39;, beta)) envelope_hat_filt = signal.filtfilt(taps, 1,envelope_hat) fig = plt.subplots(figsize=(16,4)) ax = plt.subplot(1,2,1) ax.plot(t,x,color=&#39;black&#39;,label=&#39;SIGNAL&#39;) ax.plot(t,envelope_hat,color=&#39;red&#39;,label=&#39;ENVP. EST.&#39;) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U.&#39;) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=12) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # plt.xlim(0,0.5) plt.ylim(-1,1.5) ax = plt.subplot(1,2,2) ax.plot(t,x,color=&#39;black&#39;,label=&#39;SIGNAL&#39;) ax.plot(t,envelope_hat_filt,color=&#39;red&#39;,label=&#39;ENVP. EST. FILT.&#39;) ax.set_xlabel(&#39;TIME [in s]&#39;) ax.set_ylabel(&#39;A.U.&#39;) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=12) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) # plt.xlim(0,0.5) plt.ylim(-1,1.5) plt.show() . /Users/neeks/miniconda2/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:4: WavFileWarning: Chunk (non-data) not understood, skipping it. after removing the cwd from sys.path. . Before we sum-up complete lets see how good is our filter. We will apply it on white noise and see the resulting spectrum before and after application. . # plot signal and spectrum x = np.random.normal(0,1,size=num_samples) y = signal.filtfilt(taps, 1,x) fig = plt.subplots(figsize=(16,4)) ax = plt.subplot(1,2,1) ax.plot(np.arange(0,num_samples//2+1,1)/num_samples*fs,10*np.log(abs(np.fft.rfft(x))),label=&#39;SIGNAL&#39;) ax.plot(np.arange(0,num_samples//2+1,1)/num_samples*fs,10*np.log(abs(np.fft.rfft(y))),label=&#39;SIGNAL FILTERED&#39;) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.set_xlabel(&#39;FREQUENCY [in Hz]&#39;,fontsize=14) ax.set_ylabel(&#39;SPECTRUM [in dB]&#39;,fontsize=14) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=12) plt.ylim(10,80) plt.xticks(fontsize=13) plt.yticks(fontsize=13) ax = plt.subplot(1,2,2) ax.plot(taps) ax.grid(True) ax.spines[&#39;right&#39;].set_visible(False) ax.spines[&#39;top&#39;].set_visible(False) ax.set_xlabel(&#39;taps&#39;,fontsize=14) ax.set_ylabel(&#39;A.U.&#39;,fontsize=14) ax.legend(loc=&#39;upper right&#39;,frameon=False,fontsize=12) plt.xticks(fontsize=13) plt.yticks(fontsize=13) plt.show() . No handles with labels found to put in legend. . From the plot it is clear that FIR filter does a good job in removing the spectrum beyond 50 Hz. Lets visualize the filter response and spectrum. . That&#39;s it! . To sum it up, assuming you went through the above, we now understand that . Hilbert transform can be used to estimate signal envelope | The estimation is very accurate for tone signals. In general, it is accurate for narrowband carrier signals. | The estimation performance degrades for wideband carriers, like sum of tones or white noise (broadband signal). | The estimation performance improves on applying a lowpass filter to the envelope estimate. | Such lowpass filtering of the envelope estimate can be also applied to speech signals. | . The above observations can be reasoned from a theoretical angle. I will try to do it another post. .",
            "url": "https://neerajww.github.io/myblog/2020/06/11/hilbert_transform.html",
            "relUrl": "/2020/06/11/hilbert_transform.html",
            "date": " â€¢ Jun 11, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, to know about me you can click here. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats.Â &#8617; . |",
          "url": "https://neerajww.github.io/myblog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://neerajww.github.io/myblog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}